{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "# These will be needed to prepare the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Data Problems\n",
    "\n",
    "We will deal with the following: \n",
    "- Tidy Data\n",
    "- Data Entry Problems\n",
    "- Missing Values\n",
    "\n",
    "Tidy datasets are easy to manipulate, model and visualize, and have a specific structure:\n",
    "* each variable is a column\n",
    "* each observation is a row\n",
    "* each type of observational unit is a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas.melt()**: convert DataFrame Long <-> Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  a  1  2\n",
       "1  b  3  4\n",
       "2  c  5  6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                   'B': {0: 1, 1: 3, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>myVarname</th>\n",
       "      <th>myValname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A myVarname  myValname\n",
       "0  a         B          1\n",
       "1  b         B          3\n",
       "2  c         B          5\n",
       "3  a         C          2\n",
       "4  b         C          4\n",
       "5  c         C          6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melt = pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], var_name='myVarname', value_name='myValname')\n",
    "df_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SLU12 - Support Vector Machines/data/airbnb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define elements' id**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_messy['song_id'] = range(len(df_messy))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter elements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.isin()`  \n",
    "`series.isin()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**string variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.variable = df.variable.str.lower()`  \n",
    "`df.variable = df.variable.str.strip()`  \n",
    "`df.variable = df.variable.str.replace('original', 'replaced')`  \n",
    "`df.variable = df.variable.replace({'original1': 'replaced1', 'original2': 'replaced2'})`  \n",
    "`df.variable = df.variable.str.strip()`  \n",
    "`df.variable = df.variable.str.cat()`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find duplicates using method [`duplicated()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html) and drop duplicated data with [`drop_duplicates()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html).  \n",
    "\n",
    "`duplicated_index = df.duplicated()`: Return boolean Series denoting duplicate rows, optionally only considering certain columns.  \n",
    "`df = df.drop_duplicates()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out what are the missing values, we can use method [`isnull`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html#pandas.DataFrame.isnull), followed by [`sum`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html#pandas.DataFrame.sum) to count how many missing values do we have per column.  \n",
    "\n",
    "`df.isnull()`  \n",
    "`df.isnull().sum()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the rows with at least one missing value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row, we get True if any of the columns is null and False otherwise.  \n",
    "`mask = data.isnull().any(axis=1)`  \n",
    "\n",
    "We select the rows for which the mask is True.  \n",
    "`data[mask]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping missing values**\n",
    "\n",
    "The simplest way to handle missing values is to simply discard the rows with missing values.\n",
    "\n",
    "We can do this using method [`dropna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html). This method drops all the rows with any missing values from a DataFrame.  \n",
    "\n",
    "`df = df.dropna()`: Drop the rows where at least one element is missing.  \n",
    "`df = df.dropna(axis='columns')`: Drop the columns where at least one element is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing with the mean or median**\n",
    "\n",
    "For numerical variables with missing values, we can replace the missing values with the mean or median of that variable. For this, we can use method [`fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) together with [`mean`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) or [`median`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html).  \n",
    "\n",
    "`df.variable = df.variable.fillna(df.variable.median())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro tip**\n",
    "\n",
    "We can call [`fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) with a dictionary and handle all these replacements at the same time!  \n",
    "\n",
    "`values = {'variable1': df.variable1.median(), 'variable2': df.variable2.median()}`  \n",
    "`df = df.fillna(value=values)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13232 entries, 0 to 13231\n",
      "Data columns (total 9 columns):\n",
      "room_id                 13232 non-null int64\n",
      "host_id                 13232 non-null int64\n",
      "room_type               13232 non-null object\n",
      "neighborhood            13232 non-null object\n",
      "reviews                 13232 non-null int64\n",
      "overall_satisfaction    13232 non-null float64\n",
      "accommodates            13232 non-null int64\n",
      "bedrooms                13232 non-null float64\n",
      "price                   13232 non-null float64\n",
      "dtypes: float64(3), int64(4), object(2)\n",
      "memory usage: 930.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13232, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>room_type</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>reviews</th>\n",
       "      <th>overall_satisfaction</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12148</th>\n",
       "      <td>18430120</td>\n",
       "      <td>123079348</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Lumiar</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>14035445</td>\n",
       "      <td>84062304</td>\n",
       "      <td>Private room</td>\n",
       "      <td>Parque das Nações</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040</th>\n",
       "      <td>14895450</td>\n",
       "      <td>69237427</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>São Vicente</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>124558</td>\n",
       "      <td>620702</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Santa Maria Maior</td>\n",
       "      <td>126</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9311</th>\n",
       "      <td>15269760</td>\n",
       "      <td>40814166</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Belém</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        room_id    host_id        room_type       neighborhood  reviews  \\\n",
       "12148  18430120  123079348  Entire home/apt             Lumiar        0   \n",
       "8355   14035445   84062304     Private room  Parque das Nações        7   \n",
       "9040   14895450   69237427  Entire home/apt        São Vicente       20   \n",
       "63       124558     620702  Entire home/apt  Santa Maria Maior      126   \n",
       "9311   15269760   40814166  Entire home/apt              Belém        1   \n",
       "\n",
       "       overall_satisfaction  accommodates  bedrooms  price  \n",
       "12148                   0.0             4       2.0   95.0  \n",
       "8355                    4.5             2       1.0   39.0  \n",
       "9040                    5.0             5       3.0   87.0  \n",
       "63                      4.5             6       2.0   75.0  \n",
       "9311                    0.0             4       1.0   52.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>overall_satisfaction</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.323200e+04</td>\n",
       "      <td>1.323200e+04</td>\n",
       "      <td>13232.000000</td>\n",
       "      <td>13232.000000</td>\n",
       "      <td>13232.000000</td>\n",
       "      <td>13232.000000</td>\n",
       "      <td>13232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.055081e+07</td>\n",
       "      <td>3.616444e+07</td>\n",
       "      <td>29.130063</td>\n",
       "      <td>3.284273</td>\n",
       "      <td>3.917775</td>\n",
       "      <td>1.549501</td>\n",
       "      <td>86.592352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.069884e+06</td>\n",
       "      <td>3.706975e+07</td>\n",
       "      <td>42.802762</td>\n",
       "      <td>2.123385</td>\n",
       "      <td>2.293757</td>\n",
       "      <td>1.062821</td>\n",
       "      <td>135.208926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.499000e+03</td>\n",
       "      <td>1.445500e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.054848e+06</td>\n",
       "      <td>6.197930e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.166226e+07</td>\n",
       "      <td>2.207571e+07</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.609631e+07</td>\n",
       "      <td>5.546697e+07</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.940072e+07</td>\n",
       "      <td>1.359156e+08</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7496.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            room_id       host_id       reviews  overall_satisfaction  \\\n",
       "count  1.323200e+04  1.323200e+04  13232.000000          13232.000000   \n",
       "mean   1.055081e+07  3.616444e+07     29.130063              3.284273   \n",
       "std    6.069884e+06  3.706975e+07     42.802762              2.123385   \n",
       "min    6.499000e+03  1.445500e+04      0.000000              0.000000   \n",
       "25%    5.054848e+06  6.197930e+06      2.000000              0.000000   \n",
       "50%    1.166226e+07  2.207571e+07     11.000000              4.500000   \n",
       "75%    1.609631e+07  5.546697e+07     39.000000              5.000000   \n",
       "max    1.940072e+07  1.359156e+08    438.000000              5.000000   \n",
       "\n",
       "       accommodates      bedrooms         price  \n",
       "count  13232.000000  13232.000000  13232.000000  \n",
       "mean       3.917775      1.549501     86.592352  \n",
       "std        2.293757      1.062821    135.208926  \n",
       "min        1.000000      0.000000     10.000000  \n",
       "25%        2.000000      1.000000     45.000000  \n",
       "50%        4.000000      1.000000     64.000000  \n",
       "75%        5.000000      2.000000     93.000000  \n",
       "max       16.000000     10.000000   7496.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.unique(), .nunique(), .value_counts()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max, idxmax, min, idxmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean, median, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13232.000000\n",
       "mean        29.130063\n",
       "std         42.802762\n",
       "min          0.000000\n",
       "25%          2.000000\n",
       "50%         11.000000\n",
       "75%         39.000000\n",
       "max        438.000000\n",
       "Name: reviews, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `series.skew()`, `series.kurosis()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD8CAYAAAC/1zkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE59JREFUeJzt3X/QpWV93/H3R1CBpLogi6W7kIVxx0gcjeQRabWtBQMLGJd0JMVxwsbZZjstTbDNTATrhFRlBmesqJOGhgTqQo2IaMJWSJkVMP4TfixiVCCWjVLYsJVNdwENCq759o9zPXjYPbt79tnreQ7n2fdr5sy57+993ee+zjU7z2fvnydVhSRJPbxo0h2QJC0ehookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3h066Awvt6KOPrhUrVky6G5I0Ne69996/raql47Q96EJlxYoVbNq0adLdkKSpkeT/jNvWw1+SpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG7m7Y76JNcAbwcer6rXttpRwGeBFcDDwK9U1Y4kAT4BnA08DfxaVX21rbMG+ED72A9X1fpW/wXgU8DhwC3ARVVV8/V9AFZcfPN8fvwePXz5ORPZriTtr/ncU/kUsGqX2sXAbVW1EritzQOcBaxsr3XAlfBcCF0KvAk4Bbg0yZFtnStb29n1dt2WJGmBzVuoVNVXgO27lFcD69v0euDcofq1NXAnsCTJscCZwMaq2l5VO4CNwKq27GVV9Rdt7+Taoc+SJE3IQp9TeWVVbQVo78e0+jLg0aF2W1ptb/UtI+qSpAl6oZyoz4hazaE++sOTdUk2Jdm0bdu2OXZRkrQvCx0q322Hrmjvj7f6FuC4oXbLgcf2UV8+oj5SVV1VVTNVNbN06Vg/CSBJmoOFDpUNwJo2vQa4aah+QQZOBZ5sh8duBc5IcmQ7QX8GcGtb9r0kp7Yrxy4Y+ixJ0oTM5yXFnwHeChydZAuDq7guB25IshZ4BDivNb+FweXEmxlcUvwegKranuRDwD2t3Qeravbk/7/lJ5cU/1l7SZImaN5CparetYdFp49oW8CFe/ica4BrRtQ3Aa89kD5Kkvp6oZyolyQtAoaKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4mEipJ/kOS+5N8M8lnkhyW5IQkdyV5KMlnk7yktX1pm9/clq8Y+pxLWv1bSc6cxHeRJP3EgodKkmXAbwIzVfVa4BDgfOAjwBVVtRLYAaxtq6wFdlTVq4ArWjuSnNTW+zlgFfD7SQ5ZyO8iSXq+SR3+OhQ4PMmhwBHAVuA04Ma2fD1wbpte3eZpy09Pkla/vqqeqarvAJuBUxao/5KkERY8VKrqb4CPAo8wCJMngXuBJ6pqZ2u2BVjWppcBj7Z1d7b2rxiuj1jneZKsS7IpyaZt27b1/UKSpOdM4vDXkQz2Mk4A/hHwU8BZI5rW7Cp7WLan+u7FqquqaqaqZpYuXbr/nZYkjWUSh7/eBnynqrZV1Y+ALwD/BFjSDocBLAcea9NbgOMA2vKXA9uH6yPWkSRNwCRC5RHg1CRHtHMjpwMPAHcA72xt1gA3tekNbZ62/PaqqlY/v10ddgKwErh7gb6DJGmEQ/fdpK+quivJjcBXgZ3AfcBVwM3A9Uk+3GpXt1WuBq5LspnBHsr57XPuT3IDg0DaCVxYVT9e0C8jSXqeBQ8VgKq6FLh0l/K3GXH1VlX9EDhvD59zGXBZ9w5KkubEO+olSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6GStUkrx2vjsiSZp+4+6p/Lckdyf5d0mWzGuPJElTa6xQqaq3AO8GjgM2JfnjJL84rz2TJE2dsc+pVNVDwAeA9wH/HPhkkr9K8i/nq3OSpOky7jmV1yW5AngQOA34pap6TZu+Yh77J0maIoeO2e73gD8E3l9VP5gtVtVjST4wLz2TJE2dcUPlbOAHVfVjgCQvAg6rqqer6rp5650kaaqMe07lS8DhQ/NHtNqcJFmS5MZ2TubBJP84yVFJNiZ5qL0f2domySeTbE7y9SQnD33Omtb+oSRr5tofSVIf44bKYVX1/dmZNn3EAWz3E8D/qqqfBV7P4FzNxcBtVbUSuK3NA5wFrGyvdcCVAEmOAi4F3gScAlw6G0SSpMkYN1T+bpc9hF8AfrCX9nuU5GXAPwOuBqiqZ6vqCWA1sL41Ww+c26ZXA9fWwJ3AkiTHAmcCG6tqe1XtADYCq+bSJ0lSH+OeU3kv8Lkkj7X5Y4F/NcdtnghsA/57ktcD9wIXAa+sqq0AVbU1yTGt/TLg0aH1t7TanuqSpAkZK1Sq6p4kPwu8GgjwV1X1owPY5snAb1TVXUk+wU8OdY2SUV3aS333D0jWMTh0xvHHH79/vZUkjW1/Hij5RuB1wBuAdyW5YI7b3AJsqaq72vyNDELmu+2wFu398aH2xw2tvxx4bC/13VTVVVU1U1UzS5cunWO3JUn7Mu7Nj9cBHwXewiBc3gjMzGWDVfV/gUeTvLqVTgceADYAs1dwrQFuatMbgAvaVWCnAk+2w2S3AmckObKdoD+j1SRJEzLuOZUZ4KSqGnl4aQ5+A/h0kpcA3wbewyDgbkiyFngEOK+1vYXBfTKbgadbW6pqe5IPAfe0dh+squ2d+idJmoNxQ+WbwD8EtvbYaFV9jdF7OqePaFvAhXv4nGuAa3r0SZJ04MYNlaOBB5LcDTwzW6yqd8xLryRJU2ncUPnd+eyEJGlxGPeS4j9P8jPAyqr6UpIjgEPmt2uSpGkz7tVfv87g0t8/aKVlwJ/OV6ckSdNp3PtULgTeDDwFz/1g1zF7XUOSdNAZN1SeqapnZ2eSHMoe7l6XJB28xg2VP0/yfuDw9tv0nwP+5/x1S5I0jcYNlYsZPATyG8C/YXBDor/4KEl6nnGv/vp7Bj8n/Ifz2x1J0jQbK1SSfIcR51Cq6sTuPZIkTa39efbXrMMYPJfrqP7dkSRNs7HOqVTV/xt6/U1VfRw4bZ77JkmaMuMe/jp5aPZFDPZc/sG89EiSNLXGPfz1X4amdwIPA7/SvTeSpKk27tVf/2K+OyJJmn7jHv76j3tbXlUf69MdSdI025+rv97I4Kd9AX4J+Arw6Hx0SpI0nfbnR7pOrqrvAST5XeBzVfWv56tjkqTpM+5jWo4Hnh2afxZY0b03kqSpNu6eynXA3Un+hMGd9b8MXDtvvZIkTaVxr/66LMmfAf+0ld5TVffNX7ckSdNo3MNfAEcAT1XVJ4AtSU6Ypz5JkqbUuD8nfCnwPuCSVnox8D/mq1OSpOk07p7KLwPvAP4OoKoew8e0SJJ2MW6oPFtVRXv8fZKfmr8uSZKm1bihckOSPwCWJPl14Ev4g12SpF2Me/XXR9tv0z8FvBr4naraOK89kyRNnX2GSpJDgFur6m2AQSJJ2qN9Hv6qqh8DTyd5+QL0R5I0xca9o/6HwDeSbKRdAQZQVb85L72SJE2lcUPl5vaSJGmP9hoqSY6vqkeqav1CdUiSNL32dU7lT2cnkny+54aTHJLkviRfbPMnJLkryUNJPpvkJa3+0ja/uS1fMfQZl7T6t5Kc2bN/kqT9t69QydD0iZ23fRHw4ND8R4ArqmolsANY2+prgR1V9SrgitaOJCcB5wM/B6wCfr9dqSZJmpB9hUrtYfqAJFkOnAP8UZsPcBpwY2uyHji3Ta9u87Tlp7f2q4Hrq+qZqvoOsBk4pVcfJUn7b18n6l+f5CkGeyyHt2nafFXVy+a43Y8Dv81Pnh/2CuCJqtrZ5rcAy9r0MtrPFlfVziRPtvbLgDuHPnN4HUnSBOw1VKqq++GkJG8HHq+qe5O8dbY8avP7WLa3dXbd5jpgHcDxxx+/X/2VJI1vf35PpZc3A+9I8jBwPYPDXh9n8Fyx2ZBbDjzWprcAxwG05S8Htg/XR6zzPFV1VVXNVNXM0qVL+34bSdJzFjxUquqSqlpeVSsYnGi/vareDdwBvLM1WwPc1KY3tHna8tvbE5M3AOe3q8NOAFYCdy/Q15AkjTDuzY8L4X3A9Uk+DNwHXN3qVwPXJdnMYA/lfICquj/JDcADwE7gwvZIGUnShEw0VKrqy8CX2/S3GXH1VlX9EDhvD+tfBlw2fz2UJO2PSZxTkSQtUoaKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4WPFSSHJfkjiQPJrk/yUWtflSSjUkeau9HtnqSfDLJ5iRfT3Ly0Getae0fSrJmob+LJOn5JrGnshP4rap6DXAqcGGSk4CLgduqaiVwW5sHOAtY2V7rgCthEELApcCbgFOAS2eDSJI0GQseKlW1taq+2qa/BzwILANWA+tbs/XAuW16NXBtDdwJLElyLHAmsLGqtlfVDmAjsGoBv4okaRcTPaeSZAXwBuAu4JVVtRUGwQMc05otAx4dWm1Lq+2pPmo765JsSrJp27ZtPb+CJGnIxEIlyU8DnwfeW1VP7a3piFrtpb57seqqqpqpqpmlS5fuf2clSWOZSKgkeTGDQPl0VX2hlb/bDmvR3h9v9S3AcUOrLwce20tdkjQhk7j6K8DVwINV9bGhRRuA2Su41gA3DdUvaFeBnQo82Q6P3QqckeTIdoL+jFaTJE3IoRPY5puBXwW+keRrrfZ+4HLghiRrgUeA89qyW4Czgc3A08B7AKpqe5IPAfe0dh+squ0L8xUkSaOkauRpiEVrZmamNm3aNKd1V1x8c+fevPA9fPk5k+6CpAlLcm9VzYzT1jvqJUndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKmbQyfdAb2wrbj45ols9+HLz5nIdiUdGPdUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHXjfSp6QZrU/THgPTLSgXBPRZLUjaEiSepm6kMlyaok30qyOcnFk+6PJB3MpvqcSpJDgP8K/CKwBbgnyYaqemCyPdM083ln0txN+57KKcDmqvp2VT0LXA+snnCfJOmgNdV7KsAy4NGh+S3AmybUF+mATPKKt0lx72zxmfZQyYha7dYoWQesa7PfT/KtOW7vaOBv57juYuWYjOa47G63MclHJtSTF5Zp+LfyM+M2nPZQ2QIcNzS/HHhs10ZVdRVw1YFuLMmmqpo50M9ZTByT0RyX3Tkmoy22cZn2cyr3ACuTnJDkJcD5wIYJ90mSDlpTvadSVTuT/HvgVuAQ4Jqqun/C3ZKkg9ZUhwpAVd0C3LJAmzvgQ2iLkGMymuOyO8dktEU1Lqna7by2JElzMu3nVCRJLyCGyhgO5kfBJLkmyeNJvjlUOyrJxiQPtfcjWz1JPtnG6etJTp5cz+dPkuOS3JHkwST3J7mo1Q/2cTksyd1J/rKNy39u9ROS3NXG5bPtohqSvLTNb27LV0yy//MpySFJ7kvyxTa/aMfEUNmHoUfBnAWcBLwryUmT7dWC+hSwapfaxcBtVbUSuK3Nw2CMVrbXOuDKBerjQtsJ/FZVvQY4Fbiw/Zs42MflGeC0qno98PPAqiSnAh8BrmjjsgNY29qvBXZU1auAK1q7xeoi4MGh+UU7JobKvh3Uj4Kpqq8A23cprwbWt+n1wLlD9Wtr4E5gSZJjF6anC6eqtlbVV9v09xj8sViG41JV9f02++L2KuA04MZW33VcZsfrRuD0JKNuaJ5qSZYD5wB/1ObDIh4TQ2XfRj0KZtmE+vJC8cqq2gqDP7DAMa1+0I1VOzzxBuAuHJfZwzxfAx4HNgJ/DTxRVTtbk+Hv/ty4tOVPAq9Y2B4viI8Dvw38fZt/BYt4TAyVfRvrUTACDrKxSvLTwOeB91bVU3trOqK2KMelqn5cVT/P4OkWpwCvGdWsvS/6cUnyduDxqrp3uDyi6aIZE0Nl38Z6FMxB5ruzh2/a++OtftCMVZIXMwiUT1fVF1r5oB+XWVX1BPBlBuecliSZvSdu+Ls/Ny5t+cvZ/VDrtHsz8I4kDzM4dH4agz2XRTsmhsq++SiY3W0A1rTpNcBNQ/UL2tVOpwJPzh4OWkzaMe6rgQer6mNDiw72cVmaZEmbPhx4G4PzTXcA72zNdh2X2fF6J3B7LbIb56rqkqpaXlUrGPztuL2q3s1iHpOq8rWPF3A28L8ZHB/+T5PuzwJ/988AW4EfMfhf1FoGx3hvAx5q70e1tmFwpdxfA98AZibd/3kak7cwOCTxdeBr7XW248LrgPvauHwT+J1WPxG4G9gMfA54aasf1uY3t+UnTvo7zPP4vBX44mIfE++olyR14+EvSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbv4/kwS2SECgj6EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.reviews.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit the histogram because 92% of the rooms have below 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239721886336155"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviews[df.reviews<100].count()/df.reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAGDCAYAAACBYR5jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XuUpVV95//3x+YmagAHdbBBGrWjousnKiIZk5F4A29BXVFxjCKDIa5gjImZiISIMfILTlSME2MGhQheQASVVklIS7zE/ERoEJGL/ugAQtsEVBAQFKT9zh/PrvHQnOqu7q5Tu6vq/VqrVp1nP7fvOf2s05+zaz/7pKqQJEmSNPfu17sASZIkabEyjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKkTw7gkSZLUiWFc0mZL8vdJ/nyWjvWIJD9JsqQtfznJ62bj2O14/5jk0Nk63iac951JfpjkP+b63OvVcUCSNR3P/5Ik17d/4ydN+FxHJ/nwJM8hSbMlzjMuaZwk1wIPA+4B1gFXAKcCJ1bVLzbjWK+rqi9uwj5fBj5WVZscqpK8HXh0Vf3Opu47m5LsAfz/wJ5VdVPnWg5geD1373T+fwf+uKrO7nF+Sdpa2TMuaUNeVFUPAvYEjgfeApw02ydJss1sH3MrsSfwo95BfLZt5r/XnsDlEzz+vDGT5zf1FyJJC59hXNJGVdWtVbUCeAVwaJInACT5SJJ3tse7Jvl8kh8nuTnJvya5X5KPAo8APteGKPxpkmVJKsnhSa4D/mWkbTSoPCrJBUluTXJ2kge3c91nyEWSa5M8O8lBwNHAK9r5vtXW/99hL62uY5J8L8lNSU5NslNbN1XHoUmua0NM/my61ybJTm3/H7TjHdOO/2xgJfDwVsdHxux7QJI1Sd7c6rghyWEj6+81VCfJa5N8bWS5kvx+kquS3J7kL5M8KsnXk9yW5Iwk2613zqPbc7o2yatG2rdP8u72nG9sQ5Duv16db2nDbf5hzHMZ+5q24/4EWAJ8q/WQj3sdK8mRSa4Crmptj02ysl1P303y8ta+f5L/GA2sbRjMpe3x25N8bGTd/kn+v3Ztfqv9lWD0Nb26vX7XjL4m69X39iRnJvlk2/biJE8cWf/wJGe16+CaJG8cs+/HktwGvHbM8T+S5INJzklyB/Cb011bG3q927qpa/iwDEODbkny+iRPTXJpex3+dtzzlDT3DOOSZqyqLgDWAL8xZvWb27qHMAxvOXrYpV4NXMfQy/7AqvqfI/s8A3gccOA0p3wN8N+BhzMMl3n/DGr8J+D/BT7ZzvfEMZu9tv38JvBI4IHA+uHk14HHAM8C3pbkcdOc8n8BO7XjPKPVfFgbkvM8YG2r47XT7P+f2/5LgcOBDyTZZWPPc8RBwFOA/YE/BU4EXgXsATwBeOV659q1netQ4MQkj2nr3gX8KrAP8Oi2zdvW2/fBDD3cR4yp47WMeU2r6q6qemDb5olV9agNPJcXA08D9k7yAIYPM58AHtqex98leXxVnQ/cATxzZN//1ra9lyRLgS8A72z1/wlwVpKHtHO8H3he+wvQfwEu2UB9BwOfasf5BPDZJNu2gPw54FsMr9uzgDclOXC9fc8EdgY+Ps3x/xtwHPAg4GtMc221bV/Lxq/hpwHLGT5Evw/4M+DZwOOBlyd5xgaeq6Q5YhiXtKnWMoSR9f0c2I1hfPTPq+pfa+M3pby9qu6oqp9Os/6jVXVZVd0B/DlDgJiNP9+/CnhvVV1dVT8B3gocknv3yv9FVf20qr7FELLuE+pbLa8A3lpVt1fVtcB7gFdvQi0/B97RXrNzgJ8wfAiYqXdV1W1VdTlwGfDP7XndCvwjsP7Nkn/eAvJXGELqy5ME+F3gj6rq5qq6neEDzSEj+/0COLbtO+7fayav6cb8VTv/T4EXAtdW1T9U1T1VdTFwFvDbbdvTaB80kjwIeH5rW9/vAOdU1TlV9YuqWgmsattPPa8nJLl/Vd3QXsfpXFRVZ1bVz4H3AjswfAh6KvCQqnpHVd1dVVcDH+Ler9/Xq+qzrYbprvezq+rf2j0ZP2fD19ZMXu+/rKqfVdU/M3x4Oa2qbqqq7wP/yn2vDUkdGMYlbaqlwM1j2v8aWA38c/uz/1EzONb1m7D+e8C2DD27W+rh7Xijx96GoUd/yujsJ3cy9Dyub1dguzHHWroJtfyoqu6Zwbmmc+PI45+OWR491i3tg82U7zG8Fg8BdgQuakMYfgz8U2uf8oOq+tkG6pjJa7oxo//eewJPm6qn1fQqhh56GHqmX5pke+ClwMVV9T3ua0/gZesd59eB3dpr8Qrg9cANSb6Q5LEzqa8F5jUMz3tPhuFIo+c4er3nvrFrff1tNnZtzeT13pRrQ1InC/omGUmzK8lTGcLA19Zf13pT3wy8OcnjgS8lubCqzgOm6yHfWM/5HiOPH8HQW/hDhl6+HUfqWsK9g+PGjruWIUCNHvsehrCyKbON/LDVtCfDbDNTx/r+JhxjQ+71PPllEN1cuyR5wEggfwRDb/oPGcLZ41uv6Thb8prO1Og5rge+UlXPGbth1RVJvscwFGjsEJWR43y0qn53muOcC5zbxse/k6FHe9wwLBi5HtvQlN0Znvc9wDVVtXy6J8bGX7/1t9nYtTVb17CkzuwZl7RRSX4lyQuB0xmmx/v2mG1emOTRbcjDbQzTIa5rq29kGNe6qX4nyd5JdgTeAZxZVesYpgvcIckLkmwLHANsP7LfjcCyqZvdxjgN+KMkeyV5IL8cY37PNNuP1Wo5AzguyYOS7An8MfCxDe85Y5cw9P7umOTRDGPKt9RfJNkuyW8wDAX5VOvl/RBwQpKHwjDWer0xzxszK6/piM8Dv5rk1W1c9rbtBsTRsfufAN4I/FeGsdzjfAx4UZIDkyxJskOGG1J3T/KwJL/Vxo7fxTBEaN00xwF4SpKXtqEgb2r7nA9cANyW4QbX+7fzPKF9eN0sM7i2Zvv1ltSJYVzShnwuye0MvYt/xjBO9rBptl0OfJEh0Hwd+Luq+nJb91fAMe1P+H+yCef/KPARhiEjOzAEL9p46N8HPszQU3gHw5CBKVPB7EdJLh5z3JPbsb8KXAP8DPiDTahr1B+081/N8BeDT7Tjz4YTgLsZPlycwvQ3/s3UfwC3MPSqfhx4fVV9p617C8Mwo/MzzPjxRTZt7PpsvqZTf2l5LsO467Wt9ndx7w9dpwEHAP9SVT+c5jjXM9w8eTTwA4Zr+X8w/P93P4a/5qxlGHr1DIbrajpnMwxruYVh7PZL21j/dcCLGG5+vYahV/vDDDdfbokNXVuz+npL6scv/ZEkaSOylXyRlKSFx55xSZIkqRPDuCRJktSJw1QkSZKkTuwZlyRJkjoxjEuSJEmdLMgv/dl1111r2bJlvcuQJEnSAnfRRRf9sKoesvEtx1uQYXzZsmWsWrWqdxmSJEla4Nq3AW82h6lIkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqRPDuCRJktTJNr0LmIRvf/9Wlh31hU3e79rjXzCBaiRJkqTx7BmXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqRPDuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKkTw7gkSZLUiWFckiRJ6mRiYTzJDkkuSPKtJJcn+YvWvleSbyS5Ksknk2zX2rdvy6vb+mUjx3pra/9ukgMnVbMkSZI0lybZM34X8MyqeiKwD3BQkv2BdwEnVNVy4Bbg8Lb94cAtVfVo4IS2HUn2Bg4BHg8cBPxdkiUTrFuSJEmaExML4zX4SVvctv0U8EzgzNZ+CvDi9vjgtkxb/6wkae2nV9VdVXUNsBrYb1J1S5IkSXNlomPGkyxJcglwE7AS+Hfgx1V1T9tkDbC0PV4KXA/Q1t8K/KfR9jH7jJ7riCSrkqxad+etk3g6kiRJ0qyaaBivqnVVtQ+wO0Nv9uPGbdZ+Z5p107Wvf64Tq2rfqtp3yY47bW7JkiRJ0pyZk9lUqurHwJeB/YGdk2zTVu0OrG2P1wB7ALT1OwE3j7aP2UeSJEmatyY5m8pDkuzcHt8feDZwJfAl4LfbZocCZ7fHK9oybf2/VFW19kPabCt7AcuBCyZVtyRJkjRXttn4JpttN+CUNvPJ/YAzqurzSa4ATk/yTuCbwElt+5OAjyZZzdAjfghAVV2e5AzgCuAe4MiqWjfBuiVJkqQ5MbEwXlWXAk8a0341Y2ZDqaqfAS+b5ljHAcfNdo2SJElST34DpyRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKkTw7gkSZLUiWFckiRJ6sQwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqRPDuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKkTw7gkSZLUiWFckiRJ6sQwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInEwvjSfZI8qUkVya5PMkftva3J/l+kkvaz/NH9nlrktVJvpvkwJH2g1rb6iRHTapmSZIkaS5tM8Fj3wO8uaouTvIg4KIkK9u6E6rq3aMbJ9kbOAR4PPBw4ItJfrWt/gDwHGANcGGSFVV1xQRrlyRJkiZuYmG8qm4AbmiPb09yJbB0A7scDJxeVXcB1yRZDezX1q2uqqsBkpzetjWMS5IkaV6bkzHjSZYBTwK+0ZrekOTSJCcn2aW1LQWuH9ltTWubrl2SJEma1yYexpM8EDgLeFNV3QZ8EHgUsA9Dz/l7pjYds3ttoH398xyRZFWSVevuvHVWapckSZImaaJhPMm2DEH841X1aYCqurGq1lXVL4AP8cuhKGuAPUZ23x1Yu4H2e6mqE6tq36rad8mOO83+k5EkSZJm2SRnUwlwEnBlVb13pH23kc1eAlzWHq8ADkmyfZK9gOXABcCFwPIkeyXZjuEmzxWTqluSJEmaK5OcTeXpwKuBbye5pLUdDbwyyT4MQ02uBX4PoKouT3IGw42Z9wBHVtU6gCRvAM4FlgAnV9XlE6xbkiRJmhOTnE3la4wf733OBvY5DjhuTPs5G9pPkiRJmo/8Bk5JkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqRPDuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKkTw7gkSZLUiWFckiRJ6sQwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqRPDuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUicTC+NJ9kjypSRXJrk8yR+29gcnWZnkqvZ7l9aeJO9PsjrJpUmePHKsQ9v2VyU5dFI1S5IkSXNpkj3j9wBvrqrHAfsDRybZGzgKOK+qlgPntWWA5wHL288RwAdhCO/AscDTgP2AY6cCvCRJkjSfTSyMV9UNVXVxe3w7cCWwFDgYOKVtdgrw4vb4YODUGpwP7JxkN+BAYGVV3VxVtwArgYMmVbckSZI0V+ZkzHiSZcCTgG8AD6uqG2AI7MBD22ZLgetHdlvT2qZrX/8cRyRZlWTVujtvne2nIEmSJM26iYfxJA8EzgLeVFW3bWjTMW21gfZ7N1SdWFX7VtW+S3bcafOKlSRJkubQRMN4km0ZgvjHq+rTrfnGNvyE9vum1r4G2GNk992BtRtolyRJkua1Sc6mEuAk4Mqqeu/IqhXA1IwohwJnj7S/ps2qsj9waxvGci7w3CS7tBs3n9vaJEmSpHltmwke++nAq4FvJ7mktR0NHA+ckeRw4DrgZW3dOcDzgdXAncBhAFV1c5K/BC5s272jqm6eYN2SJEnSnJhYGK+qrzF+vDfAs8ZsX8CR0xzrZODk2atOkiRJ6s9v4JQkSZI6MYxLkiRJncwojCd5wqQLkSRJkhabmfaM/32SC5L8fpKdJ1qRJEmStEjMKIxX1a8Dr2KY73tVkk8kec5EK5MkSZIWuBmPGa+qq4BjgLcAzwDen+Q7SV46qeIkSZKkhWymY8b/nyQnAFcCzwReVFWPa49PmGB9kiRJ0oI103nG/xb4EHB0Vf10qrGq1iY5ZiKVSZIkSQvcTMP484GfVtU6gCT3A3aoqjur6qMTq06SJElawGY6ZvyLwP1HlndsbZIkSZI200zD+A5V9ZOphfZ4x8mUJEmSJC0OMw3jdyR58tRCkqcAP93A9pIkSZI2YqZjxt8EfCrJ2ra8G/CKyZQkSZIkLQ4zCuNVdWGSxwKPAQJ8p6p+PtHKJEmSpAVupj3jAE8FlrV9npSEqjp1IlVJkiRJi8CMwniSjwKPAi4B1rXmAgzjkiRJ0maaac/4vsDeVVWTLEaSJElaTGY6m8plwH+eZCGSJEnSYjPTnvFdgSuSXADcNdVYVb81kaokSZKkRWCmYfztkyxCkiRJWoxmOrXhV5LsCSyvqi8m2RFYMtnSJEmSpIVtRmPGk/wucCbwv1vTUuCzkypKkiRJWgxmegPnkcDTgdsAquoq4KGTKkqSJElaDGYaxu+qqrunFpJswzDPuCRJkqTNNNMw/pUkRwP3T/Ic4FPA5yZXliRJkrTwzTSMHwX8APg28HvAOcAxkypKkiRJWgxmOpvKL4APtR9JkiRJs2BGYTzJNYwZI15Vj5z1iiRJkqRFYqZf+rPvyOMdgJcBD579ciRJkqTFY0ZjxqvqRyM/36+q9wHPnHBtkiRJ0oI202EqTx5ZvB9DT/mDJlKRJEmStEjMdJjKe0Ye3wNcC7x81quRJEmSFpGZzqbym5MuRJIkSVpsZjpM5Y83tL6q3js75UiSJEmLx6bMpvJUYEVbfhHwVeD6SRQlSZIkLQYzDeO7Ak+uqtsBkrwd+FRVvW5ShUmSJEkL3YymNgQeAdw9snw3sGzWq5EkSZIWkZn2jH8UuCDJZxi+ifMlwKkTq0qSJElaBGY6m8pxSf4R+I3WdFhVfXNyZUmSJEkL30yHqQDsCNxWVX8DrEmy14RqkiRJkhaFGYXxJMcCbwHe2pq2BT62kX1OTnJTkstG2t6e5PtJLmk/zx9Z99Ykq5N8N8mBI+0HtbbVSY7alCcnSZIkbc1m2jP+EuC3gDsAqmot8KCN7PMR4KAx7SdU1T7t5xyAJHsDhwCPb/v8XZIlSZYAHwCeB+wNvLJtK0mSJM17Mw3jd1dVMdy8SZIHbGyHqvoqcPMMj38wcHpV3VVV1wCrgf3az+qqurqq7gZOb9tKkiRJ895Mw/gZSf43sHOS3wW+CHxoM8/5hiSXtmEsu7S2pdz7C4TWtLbp2u8jyRFJViVZte7OWzezNEmSJGnuzCiMV9W7gTOBs4DHAG+rqv+1Gef7IPAoYB/gBuA9rT3jTruB9nE1nlhV+1bVvkt23GkzSpMkSZLm1kanNmzjts+tqmcDK7fkZFV148hxPwR8vi2uAfYY2XR3YG17PF27JEmSNK9ttGe8qtYBdybZ4u7mJLuNLL4EmJppZQVwSJLt25SJy4ELgAuB5Un2SrIdw02eK7a0DkmSJGlrMNNv4PwZ8O0kK2kzqgBU1Run2yHJacABwK5J1gDHAgck2YdhqMm1wO+141ye5AzgCuAe4Mj2IYAkbwDOBZYAJ1fV5ZvyBCVJkqSt1UzD+Bfaz4xV1SvHNJ+0ge2PA44b034OcM6mnFuSJEmaDzYYxpM8oqquq6pT5qogSZIkabHY2Jjxz049SHLWhGuRJEmSFpWNhfHRqQUfOclCJEmSpMVmY2G8pnksSZIkaQtt7AbOJya5jaGH/P7tMW25qupXJlqdJEmStIBtMIxX1ZK5KkSSJElabDb6pT+SJEmSJsMwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqRPDuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUifb9C5ga7LsqC9s8j7XHv+CCVQiSZKkxcCecUmSJKkTw7gkSZLUiWFckiRJ6sQwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpE8O4JEmS1MnEwniSk5PclOSykbYHJ1mZ5Kr2e5fWniTvT7I6yaVJnjyyz6Ft+6uSHDqpeiVJkqS5Nsme8Y8AB63XdhRwXlUtB85rywDPA5a3nyOAD8IQ3oFjgacB+wHHTgV4SZIkab6bWBivqq8CN6/XfDBwSnt8CvDikfZTa3A+sHOS3YADgZVVdXNV3QKs5L4BX5IkSZqX5nrM+MOq6gaA9vuhrX0pcP3Idmta23TtkiRJ0ry3tdzAmTFttYH2+x4gOSLJqiSr1t1566wWJ0mSJE3CXIfxG9vwE9rvm1r7GmCPke12B9ZuoP0+qurEqtq3qvZdsuNOs164JEmSNNvmOoyvAKZmRDkUOHuk/TVtVpX9gVvbMJZzgecm2aXduPnc1iZJkiTNe9tM6sBJTgMOAHZNsoZhVpTjgTOSHA5cB7ysbX4O8HxgNXAncBhAVd2c5C+BC9t276iq9W8KlSRJkualiYXxqnrlNKueNWbbAo6c5jgnAyfPYmmSJEnSVmFruYFTkiRJWnQM45IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOjGMS5IkSZ0YxiVJkqRODOOSJElSJ4ZxSZIkqZNtehcw3y076gubtd+1x79gliuRJEnSfGPPuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKkTw7gkSZLUiWFckiRJ6sQwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOtmmdwGL1bKjvrDJ+1x7/AsmUIkkSZJ6sWdckiRJ6sQwLkmSJHViGJckSZI6MYxLkiRJnRjGJUmSpE4M45IkSVInhnFJkiSpky5hPMm1Sb6d5JIkq1rbg5OsTHJV+71La0+S9ydZneTSJE/uUbMkSZI023r2jP9mVe1TVfu25aOA86pqOXBeWwZ4HrC8/RwBfHDOK5UkSZImYGv6Bs6DgQPa41OALwNvae2nVlUB5yfZOcluVXVDlyo78ls7JUmSFpZePeMF/HOSi5Ic0doeNhWw2++HtvalwPUj+65pbfeS5Igkq5KsWnfnrRMsXZIkSZodvXrGn15Va5M8FFiZ5Dsb2DZj2uo+DVUnAicCbL/b8vuslyRJkrY2XXrGq2pt+30T8BlgP+DGJLsBtN83tc3XAHuM7L47sHbuqpUkSZImY87DeJIHJHnQ1GPgucBlwArg0LbZocDZ7fEK4DVtVpX9gVsX43hxSZIkLTw9hqk8DPhMkqnzf6Kq/inJhcAZSQ4HrgNe1rY/B3g+sBq4Ezhs7kuWJEmSZt+ch/Gquhp44pj2HwHPGtNewJFzUJokSZI0p/wGTkmSJKmTrWmecU3A5sxNDs5PLkmSNBfsGZckSZI6MYxLkiRJnRjGJUmSpE4cM66xNmesuePMJUmSNo0945IkSVInhnFJkiSpE8O4JEmS1IlhXJIkSerEMC5JkiR1YhiXJEmSOnFqQ82azZkOcXM5jaIkSVoI7BmXJEmSOrFnXPOSX0okSZIWAnvGJUmSpE4M45IkSVInDlPRorG5N5g6vEWSJE2KPeOSJElSJ4ZxSZIkqROHqUgb4fzpkiRpUuwZlyRJkjoxjEuSJEmdGMYlSZKkThwzLs1zfhupJEnzl2Fc2orM5c2im8PgL0nS7HKYiiRJktSJYVySJEnqJFXVu4ZZt/1uy2u3Q9/XuwxJc8whMZKkuZbkoqrad3P3d8y4pAXDMe2SpPnGMC5pUdvab5r1w4IkLWyGcUnais3lhwWDvyTNPcO4JAnY/OC/OSHeIUWSNHA2FUmSJKkTZ1ORJC1oc9Vzv7nnkjS/belsKoZxSZI6MsBL85tTG0qSNI9t7TP6zKW5/GDifQvaWhjGJUnSVsEPJr/kh4XFwzAuSZI0Awv1w8JcPa+t/S8f0OcDzbwJ40kOAv4GWAJ8uKqO71ySJEnSVsMPC/PrXFPmRRhPsgT4APAcYA1wYZIVVXVF38okSZI0Ewv1w8KWmi/zjO8HrK6qq6vqbuB04ODONUmSJElbZL6E8aXA9SPLa1qbJEmSNG/Ni2EqQMa03WuC9CRHAEe0xbu+964XXjbxqjTf7Ar8sHcR2up4XWgcrwuN43WhcR6zJTvPlzC+BthjZHl3YO3oBlV1InAiQJJVWzL5uhYmrwuN43WhcbwuNI7XhcZJsmpL9p8vw1QuBJYn2SvJdsAhwIrONUmSJElbZF70jFfVPUneAJzLMLXhyVV1eeeyJEmSpC0yL8I4QFWdA5wzw81PnGQtmre8LjSO14XG8brQOF4XGmeLrotU1ca3kiRJkjTr5suYcUmSJGnBWXBhPMlBSb6bZHWSo3rXoz6S7JHkS0muTHJ5kj9s7Q9OsjLJVe33Lr1r1dxKsiTJN5N8vi3vleQb7Zr4ZLtJXItMkp2TnJnkO+1949d8v1CSP2r/h1yW5LQkO/iesfgkOTnJTUkuG2kb+/6QwftbDr00yZM3dvwFFcaTLAE+ADwP2Bt4ZZK9+1alTu4B3lxH9J0eAAAGOklEQVRVjwP2B45s18JRwHlVtRw4ry1rcflD4MqR5XcBJ7Rr4hbg8C5Vqbe/Af6pqh4LPJHhGvH9YhFLshR4I7BvVT2BYQKJQ/A9YzH6CHDQem3TvT88D1jefo4APrixgy+oMA7sB6yuqqur6m7gdODgzjWpg6q6oaoubo9vZ/iPdSnD9XBK2+wU4MV9KlQPSXYHXgB8uC0HeCZwZtvEa2IRSvIrwH8FTgKoqrur6sf4fqFhoov7J9kG2BG4Ad8zFp2q+ipw83rN070/HAycWoPzgZ2T7Lah4y+0ML4UuH5keU1r0yKWZBnwJOAbwMOq6gYYAjvw0H6VqYP3AX8K/KIt/yfgx1V1T1v2PWNxeiTwA+Af2hCmDyd5AL5fLGpV9X3g3cB1DCH8VuAifM/QYLr3h03OogstjGdMm9PFLGJJHgicBbypqm7rXY/6SfJC4Kaqumi0ecymvmcsPtsATwY+WFVPAu7AISmLXhsDfDCwF/Bw4AEMQxDW53uGRm3y/ysLLYyvAfYYWd4dWNupFnWWZFuGIP7xqvp0a75x6s9F7fdNverTnHs68FtJrmUYwvZMhp7yndufoMH3jMVqDbCmqr7Rls9kCOe+XyxuzwauqaofVNXPgU8D/wXfMzSY7v1hk7PoQgvjFwLL253O2zHcaLGic03qoI0FPgm4sqreO7JqBXBoe3wocPZc16Y+quqtVbV7VS1jeG/4l6p6FfAl4LfbZl4Ti1BV/QdwfZLHtKZnAVfg+8Vidx2wf5Id2/8pU9eF7xmC6d8fVgCvabOq7A/cOjWcZToL7kt/kjyfobdrCXByVR3XuSR1kOTXgX8Fvs0vxwcfzTBu/AzgEQxvtC+rqvVvytACl+QA4E+q6oVJHsnQU/5g4JvA71TVXT3r09xLsg/Djb3bAVcDhzF0WPl+sYgl+QvgFQwzdH0TeB3D+F/fMxaRJKcBBwC7AjcCxwKfZcz7Q/vg9rcMs6/cCRxWVas2ePyFFsYlSZKk+WKhDVORJEmS5g3DuCRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEvSViRJJXnPyPKfJHl7e3xYkjOSrEjylE045mOTXNK+6v1Rs1zvh5PsPZvHlKTFxDAuSVuXu4CXJtl1zLrDq+rlwOvZtK9rfzFwdlU9qar+fbqNkizZtFKhql5XVVds6n6SpIFhXJK2LvcAJwJ/NGZdRn7f50sikuyT5Pwklyb5TJJd2hehvQl4XZIvjdnnJ0nekeQbwK8leUqSryS5KMm5SXZL8rgkF4zssyzJpe3xl5Ps2x4/N8nXk1yc5FNJHtjaj09yRavr3a3tZUkuS/KtJF/dkhdMkuazbXoXIEm6jw8Alyb5n+u1n5TkMwzfEnnsmP1OBf6gqr6S5B3AsVX1piR/D/ykqt49Zp8HAJdV1duSbAt8BTi4qn6Q5BXAcVX135Nsl+SRVXU1wzcSnjF6kNaTfwzw7Kq6I8lbgD9O8rfAS4DHVlUl2bnt8jbgwKr6/kibJC06hnFJ2spU1W1JTgXeCPx0pP1k4ORx+yTZCdi5qr7Smk4BPjWD060DzmqPHwM8AVg5fKMzS4Ab2rozgJcDxzOE8Vesd5z9gb2Bf2v7bgd8HbgN+Bnw4SRfAD7ftv834CNJzgA+PYM6JWlBMoxL0tbpfcDFwD9MNSQ5Dpi6AfNzVfXxWTjPz6pq3dQpgMur6tfGbPdJ4FNJPg1UVV213voAK6vqlevvmGQ/4FnAIcAbgGdW1euTPA14AXBJkn2q6kez8HwkaV5xzLgkbYWq6maG3ujDR9r+rKoOaT8fX2/7W4FbkvxGa3o1w5CTTfFd4CFJfg0gybZJHt+O/+8Mveh/zhDM13c+8PQkj2777pjkV9u48Z2q6hyGsev7tPWPqqpvVNXbgB8Ce2xirZK0INgzLklbr/cw9CTP1KHA3yfZEbgaOGxTTlZVdyf5beD9bdjLNgw99Je3TT4J/DWw15h9f5DktcBpSbZvzccAtwNnJ9mBofd86sbUv06yvLWdB3xrU2qVpIUiVfe5IV+SJEnSHHCYiiRJktSJYVySJEnqxDAuSZIkdWIYlyRJkjoxjEuSJEmdGMYlSZKkTgzjkiRJUieGcUmSJKmT/wP6xrx0bACE+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.reviews.plot.hist(bins=np.arange(0,101,2), xlim=(0,100), figsize=(12, 6))\n",
    "plt.xlabel('Nº of reviess')\n",
    "plt.title('Distribution of number of revies per room');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance, covariance, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = df.reviews\n",
    "s2 = df.accommodates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.261321086741796"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.293756980750532"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**covariance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7197875496125646"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.cov(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**correlation**\n",
    "\n",
    "In the broadest sense correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related (Pearson correlation).\n",
    "\n",
    "There are multiple reasons to use correlation instead of covariance: \n",
    "> 1. Correlation is adimensional (unit free), so it's easier to think about (none of this \"cm.years\" nonsense) \n",
    "> 2. The value of correlation takes place between -1 and +1, while covariance lies between $-\\infty$ and $+\\infty$.\n",
    "> 3. Correlation is not affected by the change in scale, while covariance is. \n",
    "\n",
    "\n",
    "Correlation is just normalized covariance. Normalized by what? By variance! \n",
    "\n",
    "$$ correlation = \\frac{covariance(X, Y)}{\\sqrt{Var(X) * Var(Y)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0073313699308307475"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.corr(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative measure is **Spearman correlation**. It is computed using the ranking position of the variable values. The easiest way to think of this, is to forget about the actual values, and consider only their relative positions (ranks).\n",
    "Spearman correlation is **much more resistant to outliers** and data entry problems than Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08882378299880923"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.corr(s2, method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas DataFrame correlation method to compute de correlation table for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>overall_satisfaction</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>room_id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.595750</td>\n",
       "      <td>-0.496113</td>\n",
       "      <td>-0.308704</td>\n",
       "      <td>-0.057226</td>\n",
       "      <td>-0.030635</td>\n",
       "      <td>-0.068688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_id</th>\n",
       "      <td>0.595750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.286024</td>\n",
       "      <td>-0.179504</td>\n",
       "      <td>-0.071530</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.048030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews</th>\n",
       "      <td>-0.496113</td>\n",
       "      <td>-0.286024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.431931</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>-0.043997</td>\n",
       "      <td>-0.084228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall_satisfaction</th>\n",
       "      <td>-0.308704</td>\n",
       "      <td>-0.179504</td>\n",
       "      <td>0.431931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078501</td>\n",
       "      <td>0.019409</td>\n",
       "      <td>-0.126716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>-0.057226</td>\n",
       "      <td>-0.071530</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.078501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791061</td>\n",
       "      <td>0.300386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>-0.030635</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.043997</td>\n",
       "      <td>0.019409</td>\n",
       "      <td>0.791061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.302463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>-0.068688</td>\n",
       "      <td>-0.048030</td>\n",
       "      <td>-0.084228</td>\n",
       "      <td>-0.126716</td>\n",
       "      <td>0.300386</td>\n",
       "      <td>0.302463</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       room_id   host_id   reviews  overall_satisfaction  \\\n",
       "room_id               1.000000  0.595750 -0.496113             -0.308704   \n",
       "host_id               0.595750  1.000000 -0.286024             -0.179504   \n",
       "reviews              -0.496113 -0.286024  1.000000              0.431931   \n",
       "overall_satisfaction -0.308704 -0.179504  0.431931              1.000000   \n",
       "accommodates         -0.057226 -0.071530  0.007331              0.078501   \n",
       "bedrooms             -0.030635 -0.050984 -0.043997              0.019409   \n",
       "price                -0.068688 -0.048030 -0.084228             -0.126716   \n",
       "\n",
       "                      accommodates  bedrooms     price  \n",
       "room_id                  -0.057226 -0.030635 -0.068688  \n",
       "host_id                  -0.071530 -0.050984 -0.048030  \n",
       "reviews                   0.007331 -0.043997 -0.084228  \n",
       "overall_satisfaction      0.078501  0.019409 -0.126716  \n",
       "accommodates              1.000000  0.791061  0.300386  \n",
       "bedrooms                  0.791061  1.000000  0.302463  \n",
       "price                     0.300386  0.302463  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation - bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The bias-variance tradeoff**  \n",
    "\n",
    "There is a trade-off between the bias (or underfitting) and variance (or overfitting). When increasing model complexity, bias is reduced while variance increases.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split (aka holdout method)  \n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "```\n",
    "---\n",
    "\n",
    "The best possible solution is to leave a random subset of the data aside from the beginning to test our final model at the end.\n",
    "\n",
    "<img src=\"SLU09 - Model Selection and Overfitting\\media\\test_set.png\" width=\"300\">\n",
    "\n",
    "*Fig.: Test set illustrated, you holdout a significant chunk of the data for testing your model in the end*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method we can compute **two types evaluation metrics**:\n",
    "1. In-sample-error (ISE) or **training error**: how well our model performs on the training data.\n",
    "2. Out-of-sample error (OSE) or **testing error**: how well the model performs on previsouly unseen data and if it's picking up patterns that generalize well.  \n",
    "\n",
    "Ideally, both training and test errors are low and close to one another.  \n",
    "\n",
    "* *Underfitted* models tend to perform poorly on both train and test data, having large (and similar) in-sample- and out-of-sample errors.\n",
    "\n",
    "* *Overfitting* is detected when a model that performs on training data but not quite so well in the test set: the bigger the gap, the greater the overfitting.  \n",
    "\n",
    "Underfitted and overfitted examples:\n",
    "> SuperConservative model error:  \n",
    "> Train 12.67% | Test 10.5%  \n",
    "---  \n",
    "> WellBalanced model error:  \n",
    "> Train:   7.0% | Test: 8.5%  \n",
    "---  \n",
    "> SuperFlexible model error:  \n",
    "> Train:   0.0% | Test: 11.25%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set (fixed set)  \n",
    "\n",
    "----\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25)\n",
    "del X_temp, y_temp\n",
    "```\n",
    "---\n",
    "\n",
    "<img src=\"SLU09 - Model Selection and Overfitting\\media\\validation_set.png\" width=\"300\">\n",
    "\n",
    "*Fig.: Validation set as compared with the holdout approach*\n",
    "\n",
    "You might be wondering, how is this validation set different from the test set? Typically, a validation set will be used to tune parameters of the model, and then final evaluation of OSE will be done on the test set. \n",
    "\n",
    "We can use the validation set we created above to find the optimal value of *k* for our KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set (k-Fold Cross-validation)\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=classification_error)\n",
    "mean_error = round(np.mean(scores), 2)\n",
    "var_error = round(np.var(scores), 2)\n",
    "```\n",
    "---  \n",
    "\n",
    "Test error results can be subject to great variability, especially for smaller datasets, depending on how we split the data (i.e., which observations go where).\n",
    "\n",
    "Also, and quite obviously, holding out *more* data reduces the amount available for training, possibly leading us to *overestimate* the test error.\n",
    "\n",
    "<img src=\"SLU09 - Model Selection and Overfitting\\media\\cross_validation.png\" width=\"600\">\n",
    "\n",
    "This way, we use every observation to both train and test out model: each fold is used once as validation, while the *k*-1 remaining folds form the training set. The mean of the error of every fold can be seen as a proxy for OSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What evaluation metric to use?  \n",
    "\n",
    "The `scikit-learn` package includes most of the **validation metrics for classification** we will ever use (which can mostly found [here](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)). \n",
    "\n",
    "The validation metrics for classification presented in this notebook are:\n",
    "- Accuracy Score\n",
    "- Confusion Matrix\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- Area Under the Receiver Operating Charactristic curve (AUROC)\n",
    "\n",
    "\n",
    "<img src=\"score_metrics.PNG\" width=\"800\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sklearn metrics:\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "                            recall_score, f1_score, roc_auc_score, roc_curve, \\\n",
    "                            confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix   \n",
    "---  \n",
    "```python  \n",
    "# this is scikit-learn's confusion matrix, we will use it a lot! \n",
    "confmat = confusion_matrix(y_true, y_pred)  \n",
    "```  \n",
    "---  \n",
    "\n",
    "or\n",
    "\n",
    "---\n",
    "```python\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "```\n",
    "---  \n",
    "\n",
    "In binary classification, we either predicted 0 or 1, and were either right, or wrong. This matrix covers all cases.  \n",
    "\n",
    "<img src=\"SLU11 - Metrics for Classification/data/confusion_mat_labelled.png\" width=\"400\">  \n",
    "\n",
    "Deconstructing the logic: \n",
    "- The word ***\"positive\"*** or ***\"negative\"*** refers to what we **predicted**. \n",
    "- The word ***\"true\"*** or ***\"false\"*** refers to the real **observed class**.   \n",
    "\n",
    "Reading **horizontal**: *class 0* and *class 1* **observations**.  \n",
    "Reading **vertical**: *class 0* and *class 1* **predictions**.  \n",
    "Reading **diagonal**: correct predictions ($\\searrow$) ans wrong predictions ($\\swarrow$).  \n",
    "**Sum of squares**: total number of observations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of scores - static evaluation   \n",
    "\n",
    "An usual behaviour for a binary classification algorithm is to predict the probability of each observation to be of *class 1* (e.g. `predict_proba(self, X)` [method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) in `sklearn.linear_model.LogisticRegression`) and then decide on the actual classification based on a previously set threshold ranging from 0 to 1. **For a given algorithm and a given threshold evaluation can be done using one of the following metrics**.\n",
    "\n",
    "**Accuracy** is the total fraction of correct predictions.\n",
    "\n",
    "$$ A = \\frac{Number\\ of\\ correct\\ predictions}{Total\\ number\\ of\\ predictions} = \\frac{TP + TN}{TP + TN + FP + FN}  $$  \n",
    "\n",
    "The **problem with accuracy**: If we have a more realistic dataset, where only ~0.4% of the population has *class 1* and we predict *class 0* for all observations we will get Accuracy > 99,6%.  \n",
    "\n",
    "Conclusion: \n",
    "> **Accuracy is dangerous, as it is intuitive, but only works on well balanced datasets**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** is the ability to be a good classifier on the positive prediction. Appropriate when the cost of false positives is high. Leads to **higher decision thresholds**.  \n",
    "\n",
    "$$ P = \\frac{True\\ Positive}{Positive\\ predictions} = \\frac{TP}{TP+FP} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/2ombhi.jpg\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** is the ability to be a good classifier on the positive observations. Appropriate when the cost of false negatives is high. Leads to **lower decision thresholds**.  \n",
    "\n",
    "$$ R = \\frac{True\\ Positive}{Positive\\ observations} = \\frac{TP}{TP+FN} $$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/2ombqy.jpg\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 score** is an attempt to find a \"midground\" between precision and recall. However it should be used with caution, as it assumes that both False Positives and False Negatives have the same impact on the end objective, which in a real scenario is rarely true.   \n",
    "\n",
    "$$ F1 = 2\\frac{P \\times R}{P+R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve - dynamic evaluation   \n",
    "\n",
    "(see the Wiki page for [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity))\n",
    "\n",
    "For a given clssification algorithm, **increasing the decision threshold** (from 0 to 1):  \n",
    "\n",
    "- **Decreases** (from 1 to 0)\n",
    " - the <font color='green'>**true**</font> positive rate (or recall, $TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}$ ) - which is <font color='red'>**bad**</font>.\n",
    " - the <font color='red'>**false**</font> positive rate ($FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN} $) - which is <font color='green'>**good**</font>.   \n",
    " \n",
    " \n",
    "- **Increases** (from 0 to 1)\n",
    " - the <font color='green'>**true**</font> negative rate ($TNR = \\frac{TN}{N} = \\frac{TN}{TN + FP}$ ) - which is <font color='green'>**good**</font>.\n",
    " - the <font color='red'>**false**</font> negative rate ($FNR = \\frac{FN}{P} = \\frac{FN}{FN + TP}$ ) - which is <font color='red'>**bad**</font>. \n",
    " \n",
    "\n",
    "So **at low thresholds**, we have **lots of 1s (<font color='red'>False Positives</font>)**, and **at high thresholds** we have **lots of 0s (<font color='red'>False Negatives</font>)**.  \n",
    "Also, **at low thresholds**, we have **lots of 1s (<font color='green'>True Positives</font>)**, and **at high thresholds** we have **lots of 0s (<font color='green'>False Negatives</font>)**.\n",
    "\n",
    "\n",
    "The challenge here is to find a good **balance** between **higher TPR and TNR** and **lower FPR and FNR**.\n",
    "\n",
    "<img src=\"https://www.medcalc.org/manual/_help/images/roc_intro2.png\" width=\"300\">  \n",
    "\n",
    "<img src=\"https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0163111.g004&type=large\" width=\"600\">  \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "What is our TPR (True Positive Rate) and FPR (False Positive Rate) for each of these thresholds?  \n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a very common (and important) metric for **binary classification problems** (when predicting 0 or 1).  \n",
    "\n",
    "**Formally**, it is created by plotting the fraction of true positives out of the positives (TPR = true positive rate, a.k.a., sensitivity) vs. the fraction of false positives out of the negatives (FPR = false positive rate, or 1-specificity), at various threshold settings. Sklearn already provide us with all the tools:\n",
    "\n",
    "In the following images we've drawn a perfect and random classifier overlayed with the original (ok-ish) one. \n",
    "\n",
    "<img src=\"SLU11 - Metrics for Classification/data/rocs.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sufficiency and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **usual loss function has a (serious) problem: it is prone to overfitting** because the optimization methods will adapt, as much as they can, the parameters to the training set.\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$\n",
    "\n",
    "This problem can be tackled by means of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization:  \n",
    "**1. Ridge** (L2 norm): \n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_2 \\|\\beta\\|_2^2 = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_2 \\sum_{k=1}^K \\beta_k^2$$  \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "clf = RidgeClassifier(normalize=True, alpha=0.0001, random_state=10)\n",
    "clf.fit(X, y)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l2', C=1.0)\n",
    "clf.fit(X, y)\n",
    "```\n",
    "\n",
    "\n",
    "**2. Lasso** (L1 norm):  \n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\|\\beta\\|_1^1 = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\sum_{k=1}^K \\left|\\beta_k\\right|$$\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l1', C=1.0)\n",
    "clf.fit(X, y)\n",
    "```\n",
    "\n",
    "\n",
    "**3. Elastic Net** (L1+L2 norm):  \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='elasticnet', C=1.0, l1_ratio=1)\n",
    "clf.fit(X, y)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Regularization awards a model's *goodness of fit* while penalizing *model complexity* automatically, while it is fitting the data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
