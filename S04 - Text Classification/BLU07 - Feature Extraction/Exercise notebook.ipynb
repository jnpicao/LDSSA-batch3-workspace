{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12bea12324c032d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jnpicao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f07b8631beb0508c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q1. Country names\n",
    "\n",
    "For the first question you will be making use of regex. In particular you have a list of countries and you'll have to answer some very specific questions about that list.\n",
    "\n",
    "Start by loading the defining the path to this list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe61fa6cbbdef77d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/countries.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52faf4f77a990570",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The first thing you will build is a wrapper that will apply a regex pattern into a given file, and return a list of results found matching that pattern. Implement it in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a88528290dd35f7d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_all_in_file(pattern, path):\n",
    "    \"\"\"\n",
    "    Function that returns all matches of a certain pattern in a certain text.\n",
    "    \n",
    "    Args:\n",
    "    pattern - regex pattern\n",
    "    path - path to the file\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pattern_match_list = []\n",
    "    \n",
    "    f = open(path, 'r')\n",
    "\n",
    "    # create a list of lines\n",
    "    lines_list = f.readlines()\n",
    "\n",
    "    #loop all lines(lines_list)    \n",
    "    for line in lines_list:\n",
    "        #print(line)\n",
    "        for match in re.findall(pattern, line):\n",
    "            #print(match)\n",
    "            pattern_match_list.append(match)      \n",
    "\n",
    "    # closing the file\n",
    "    f.close()\n",
    "    \n",
    "    return pattern_match_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef7991b98e752180",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure this function is working with the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a056266aa5ada57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_all_in_file(pattern=\"^P.+?$\", path=path)[8] == \"Portugal\"\n",
    "assert find_all_in_file(pattern=\"^.+?a$\", path=path)[18] == \"Croatia\"\n",
    "assert len(find_all_in_file(pattern=\"^.+?ca$\", path=path)) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df341d1f1838c29e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.a)\n",
    "\n",
    "Now that you prepared your wrapper, let's move on to the actual expressions. The first thing we are looking for is for countries with loooong names. In particular we want you to find all the countries with more than 15 letters. Use the wrapper you defined above and assign its return to a variable `ret`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cddd8e2e48afb31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ret_long = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_long = find_all_in_file(pattern=\"^.{15,}$\", path=path)\n",
    "len(ret_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5a358ed83c473214",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries with more than 15 or more letters:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries with more than 15 or more letters: \", len(ret_long))\n",
    "assert len(ret_long) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6727f8213243afc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "Now, find out how many countries:\n",
    "* Start with a vowel\n",
    "* Start with a consonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-447eb675481e47fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_vowel = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_vowel = find_all_in_file(pattern=\"^[AEIOUaeiou].+$\", path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-535072609d84d577",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries that start with vowels:  36\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries that start with vowels: \" , len(ret_vowel))\n",
    "assert len(ret_vowel) == 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66af8912995d91a2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_consonant = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_consonant = find_all_in_file(pattern=\"^[^AEIOUaeiou].+$\", path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1c17b943b63b52ae",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries that start with consonants:  160\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries that start with consonants: \" , len(ret_consonant))\n",
    "assert len(ret_consonant) == 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e3cc736b933d71e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.c)\n",
    "\n",
    "Next, find how many countries are composed by only one word and end in `ia`. You'll want to have a list with countries such as `Croatia`, `Serbia`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bf14a2df3dd3e4e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ret_ia = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_ia = find_all_in_file(pattern=\"^\\S*ia$\", path=path)\n",
    "len(ret_ia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-19b61469674be299",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants of countries ending in \"ia\":  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variants of countries ending in \\\"ia\\\": \" , len(ret_ia))\n",
    "assert \"Serbia\" in ret_ia\n",
    "assert \"Croatia\" in ret_ia\n",
    "assert len(ret_ia) == 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6bb573c7736db8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.d)\n",
    "\n",
    "Finally, find the countries which have at least four consecutive consonants, without taking into account the first letter (Hint: you can assume the first letter is capitalized). So, it should match things like `Abcdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e92132a5b63400f3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Egypt', 'Kazakhstan', 'Kyrgyzstan']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ret_bcdf = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_bcdf = find_all_in_file(pattern=\"^[A-Za-z].*[^AEIOUaeiou\\s]{4,}.*$\", path=path)\n",
    "ret_bcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5150c254b592778e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries matched:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries matched: \" , len(ret_bcdf))\n",
    "assert len(ret_bcdf) == 3\n",
    "assert hashlib.sha256(' '.join(ret_bcdf).encode()).hexdigest() == '7da1a15074b9245ae3b88fb92fc5c484243003a084d03280bf11d9346d768869'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0409818769bbf3d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2. A Study in Scarlet\n",
    "\n",
    "For this following questions we will be looking at Sir Arthur Conan Doyle's [\"A Study in Scarlet\"](https://en.wikipedia.org/wiki/A_Study_in_Scarlet) (which you might have seen adapted to tv in [\"A Study in Pink\"](https://en.wikipedia.org/wiki/A_Study_in_Pink)). We will be performing common preprocessing operations on this text, as it is a common task in Natural Language Processing. Start by downloading the data and loading it into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abce95c72c255d16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/sherlock.txt\"\n",
    "data =  [line.strip('\\n') for line in open(path, 'r', encoding='utf8') if len(line)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-addc0c904c359402",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.a)\n",
    "\n",
    "First tokenize the data. Implement the function to receive an NLTK-style tokenizer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b7930a61806e32",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_tokenizer(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with the tokens of given text. I.e\n",
    "    for an input ['Abc def', 'Ghi jkl mn'] it returns [['Abc', 'def'], ['Ghi', 'jkl', 'mn']]\n",
    "    \n",
    "    Args:\n",
    "    data - list with the data\n",
    "    tokenizer - nltk tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    tokens_list = []\n",
    "    for line in data:\n",
    "        tokens_list.append(tokenizer.tokenize(line))\n",
    "        \n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f829c5a222c54690",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "data_tok = apply_tokenizer(data=data, tokenizer=tokenizer)\n",
    "\n",
    "assert len(data_tok) == 3770\n",
    "assert len([w for s in data_tok for w in s]) == 51648\n",
    "assert data_tok[8] == ['I','could','join','it',',','the','second','Afghan','war','had','broken','out','.','On','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b76d100b971a777f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.b)\n",
    "\n",
    "The second step you will implement is lowercasing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee47fb5a45fbd622",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_lowercase(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with all the tokens lowecased.\n",
    "    \n",
    "    Args:\n",
    "    data - list with tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    data_lower = []\n",
    "    \n",
    "    for list_of_words in data:\n",
    "        data_lower.append([w.lower() for w in list_of_words])\n",
    "        \n",
    "    return data_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7979e12840663ea2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_lc = apply_lowercase(data=apply_tokenizer(data=data, tokenizer=tokenizer))\n",
    "\n",
    "assert len(data_lc) == 3770\n",
    "assert len([w for s in data_lc for w in s]) == 51648\n",
    "assert data_lc[8] == ['i','could','join','it',',','the','second','afghan','war','had','broken','out','.','on','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8044c14c20583cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.c)\n",
    "\n",
    "Now implement a function that filters the stopwords.\n",
    "\n",
    "NOTE: Stopwords adapted from [here](https://gist.github.com/sebleier/554280). (Notice what we added some specific things, like ?\" and .\" to the stopwords. This was shown to be a limitation of the nltk tokenizer so it will be removed that way, instead of the more conventional way. This goes to show that there are more powerful tokenizers that you should use in the case you have to perform tokenization in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ecb29c8fe117f1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_stopwords(data, stopwords_fp):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no stopwords.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stopwords_fp - path to the stopwords file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the list of stopwords from the file\n",
    "    # stopwords = ...\n",
    "    # YOUR CODE HERE\n",
    "    stopwords = [line.strip(' .\\n') for line in open(stopwords_fp, 'r', encoding='utf8') if len(line)>1]\n",
    "    \n",
    "#     # Alternative solution\n",
    "#     # create a file object f\n",
    "#     f = open(stopwords_fp, 'r', encoding='utf8' )\n",
    "\n",
    "#     # create a list of lines\n",
    "#     lines_list = list(f)\n",
    "    \n",
    "#     stopwords = []    \n",
    "#     for line in lines_list:\n",
    "#         stopwords.append(line.strip(' .\\n'))    \n",
    "    \n",
    "    # Filter the stopwords from the text\n",
    "    # data_filt = ...\n",
    "    # YOUR CODE HERE\n",
    "    data_filt = []\n",
    "    for list_of_words in data:\n",
    "        data_filt.append([w for w in list_of_words if w not in stopwords])\n",
    "    \n",
    "    return data_filt\n",
    "    \n",
    "    return data_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f70da0255ea6e291",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords_fp = \"data/english_stopwords.txt\"\n",
    "data_filt_sw = apply_filter_stopwords(data=apply_lowercase(apply_tokenizer(data, tokenizer)), \n",
    "                                      stopwords_fp=stopwords_fp)\n",
    "assert len(data_filt_sw) == 3770\n",
    "assert len([w for s in data_filt_sw for w in s]) == 27733\n",
    "assert data_filt_sw[8] == ['could', 'join', ',', 'second', 'afghan', 'war', 'broken', '.', 'landing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d0cb317596faa2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.d)\n",
    "\n",
    "After filtering stopwords, we want to remove punctuation from the text as well. Make use of `string.punctuation` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0cd3cf5cc97a8f2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_punkt(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no punctuation.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    data_no_punct = []\n",
    "    for list_of_words in data:\n",
    "        data_no_punct.append([w for w in list_of_words if w not in string.punctuation])\n",
    "    \n",
    "    return data_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-650a677a3a01d1bf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_filt_punkt = apply_filter_punkt(data=apply_tokenizer(data, tokenizer))\n",
    "\n",
    "assert len(data_filt_punkt) == 3770\n",
    "assert len([w for s in data_filt_punkt for w in s]) == 46362\n",
    "assert data_filt_punkt[8] == ['I','could','join','it','the','second','Afghan','war','had','broken','out','On','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38d66dd89731e114",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.e)\n",
    "\n",
    "The last preprocessing step you are going to implement is stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a831ba989f3e50e6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_stemmer(data, stemmer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with stemmed data.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stemmer - instance of stemmer to use\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    data_stemmed = []\n",
    "    for list_of_words in data:\n",
    "        data_stemmed.append(list(map(stemmer.stem, list_of_words)))\n",
    "    \n",
    "    return data_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3596a6510ebbda3d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "data_stems = apply_stemmer(data=apply_lowercase(apply_tokenizer(data, tokenizer)),\n",
    "                           stemmer=stemmer)\n",
    "\n",
    "assert len(data_stems) == 3770\n",
    "assert len([w for s in data_stems for w in s]) == 51648\n",
    "assert data_stems[8][-2] == 'land'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bfe5aed6ebdbb26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.f)\n",
    "\n",
    "Finally, join everything in a function, that applies the steps in the following order, in :\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Filtering stopwords\n",
    "* Filtering punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea5b2305431c20dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list, lower=True, remove_punct=True, stopwords=[]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, sentences):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        # sentences_tokens = ...\n",
    "        # YOUR CODE HERE\n",
    "        sentences_tokens = apply_tokenizer(sentences, self.tokenizer)\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_lowercase(sentences_tokens)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_filter_punkt(sentences_tokens)\n",
    "\n",
    "        if self.stopwords:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_filter_stopwords(sentences_tokens, self.stopwords)\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_stemmer(sentences_tokens, self.stemmer)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentences_prep = [\" \".join(tokens).strip() for tokens in sentences_tokens]\n",
    "        return sentences_prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4a87d0c9b1f20f7e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "data_preprocessed = text_cleaner.clean_sentences(data)\n",
    "assert len(data_preprocessed) == 3770\n",
    "assert len([w for s in data_preprocessed for w in s.split()]) == 22447\n",
    "assert data_preprocessed[8] == 'could join second afghan war broken land'\n",
    "assert data_preprocessed[15] == 'noth misfortun disast remov brigad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f48abbdab8acc3d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3. Movie reviews\n",
    "\n",
    "We will now use what we've learned to explore movie reviews. We will start by analysing the dataset, then we will apply the preprocessing you implemented above, and finally we will see how it affects a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d6a39b05ecd310c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.a)\n",
    "\n",
    "To get some stats on the dataset, we will start by implementing your own function to get the list of n-grams from a list of tokens. Complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43601c5f562f5867",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"\n",
    "    Returns list of tuples for all the n-grams\n",
    "    \n",
    "    Args:\n",
    "    data - list of tokenized data (flattened)\n",
    "    n - the n in n-grams\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    ngram_list = []\n",
    "    for idx in range(len(data)-n+1):\n",
    "        idx_ngram = tuple(data[idx:idx+n])\n",
    "        ngram_list.append(idx_ngram)\n",
    "    \n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f601c9c7d4b35092",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ngrams(\"The actress won the oscar\".split(), 2) == [('The', 'actress'), ('actress', 'won'), ('won', 'the'), ('the', 'oscar')]\n",
    "assert ngrams(\"The actress won the oscar\".split(), 3) == [('The', 'actress', 'won'), ('actress', 'won', 'the'), ('won', 'the', 'oscar')]\n",
    "assert ngrams(\"The actress won the oscar\".split(), 4) == [('The', 'actress', 'won', 'the'), ('actress', 'won', 'the', 'oscar')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cdc8657fcd5f3c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.b)\n",
    "\n",
    "We will now see in our dataset what are the most common n-grams. Load the data and find how many unique bi-grams, tri-grams and four-grams we have. Also, take advantage of `Counter` and `most_common()` to find the most common tri-gram. Merge together the words of the most common trigram to get one single string. (Hint: look at python's `join` function, exemplefied below when joining the full text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21e130445a725501",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text and split into full list of words\n",
    "docs = df['text']\n",
    "full_text = ' '.join([d.strip() for d in docs])\n",
    "words = full_text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caa9453210806809",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement below the code to get the sets of unigrams, bigrams, trigrams and fourgrams, and to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-539dba0b0aabb366",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# unigrams = ...\n",
    "# bigrams = ...\n",
    "# trigrams = ...\n",
    "# fourgrams = ...\n",
    "# most_common_trigram = ...\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "unigrams = Counter(ngrams(words, 1))\n",
    "bigrams = Counter(ngrams(words, 2))\n",
    "trigrams = Counter(ngrams(words, 3))\n",
    "fourgrams = Counter(ngrams(words, 4))\n",
    "most_common_trigram = ' '.join(trigrams.most_common()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91094315aa4e5abf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102712 unigrams\n",
      "Found 544165 bigrams\n",
      "Found 962749 trigrams\n",
      "Found 1132643 fourgrams\n",
      "Most common trigram is \"one of the\"\n"
     ]
    }
   ],
   "source": [
    "n_unigrams = str(len(unigrams))\n",
    "n_bigrams = str(len(bigrams))\n",
    "n_trigrams = str(len(trigrams))\n",
    "n_fourgrams = str(len(fourgrams))\n",
    "\n",
    "print('Found {} unigrams'.format(n_unigrams))\n",
    "assert hashlib.sha256(n_unigrams.encode()).hexdigest() == '1ae2d8247d3ad491c79aed034828ba78b21e25438a6e9a61f252eb566e39e877'\n",
    "\n",
    "print('Found {} bigrams'.format(n_bigrams))\n",
    "assert hashlib.sha256(n_bigrams.encode()).hexdigest() == '7d2d487bcdf890f05578da49f574e3e8f22f7420f752071a24eb49759de5adf8'\n",
    "\n",
    "print('Found {} trigrams'.format(n_trigrams))\n",
    "assert hashlib.sha256(n_trigrams.encode()).hexdigest() == '8c54e3c7087ab053a77d56c60408fd47837081fdea817b7cc9e68f134cef969d'\n",
    "\n",
    "print('Found {} fourgrams'.format(n_fourgrams))\n",
    "assert hashlib.sha256(n_fourgrams.encode()).hexdigest() == '8df23d7f0d27298e7a7f77bdce4d15bb401098175c36514ba94b5350177b1593'\n",
    "\n",
    "print('Most common trigram is \"{}\"'.format(most_common_trigram))\n",
    "assert hashlib.sha256(most_common_trigram.encode()).hexdigest() == '28b6f04107ef3f1120975abf58ca8d08d20243beea929999b203f0add941fe16'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2477efb5c417c3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.c)\n",
    "\n",
    "Let's now process a sample of our dataset with the previous Q2 preprocessing, and get a Bag of Words representation. Start by using your text cleaner to get a preprocessed version of this dataset.\n",
    "\n",
    "Note: if you didn't finish the text cleaner above, jump to the TF-IDF implementation directly, where you can load the BoW from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-306c1207ab2a8c1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=None,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "docs_preprocessed = text_cleaner.clean_sentences(docs[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11aa60fa130d8eeb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can get a vocabulary, vectorize our dataset and convert it into a BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23a2f9da897ab002",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>/&gt;&lt;</th>\n",
       "      <th>/&gt;</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>.&lt;</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>endearing</th>\n",
       "      <th>cortney</th>\n",
       "      <th>fatal</th>\n",
       "      <th>incidents</th>\n",
       "      <th>erupts</th>\n",
       "      <th>semblance</th>\n",
       "      <th>thirds</th>\n",
       "      <th>miserable</th>\n",
       "      <th>shoes</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   br  /><  />  movie  film  .<  one  like  time  even  ...   endearing  \\\n",
       "0   0    0   0      3     5   0    1     0     2     0  ...           0   \n",
       "1   0    0   0      1     0   0    0     0     0     0  ...           0   \n",
       "2   5    2   4      6     0   2    2     1     0     1  ...           0   \n",
       "3   0    0   0      4     0   0    1     1     1     1  ...           0   \n",
       "4   8    3   6      1     0   2    2     0     1     0  ...           0   \n",
       "\n",
       "   cortney  fatal  incidents  erupts  semblance  thirds  miserable  shoes  \\\n",
       "0        0      0          0       0          0       0          0      0   \n",
       "1        0      0          0       0          0       0          0      0   \n",
       "2        0      0          0       0          0       0          0      0   \n",
       "3        0      0          0       0          0       0          0      0   \n",
       "4        0      0          0       0          0       0          0      0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 7442 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(docs):\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "def vectorize(docs):\n",
    "    vocabulary = build_vocabulary(docs)\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocabulary])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return (vocabulary, vectors)\n",
    "\n",
    "def build_df(docs):\n",
    "    vocab, vectors = vectorize(docs)\n",
    "    return pd.DataFrame(vectors, columns=vocab)\n",
    "\n",
    "BoW = build_df(docs_preprocessed)\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9951e7e69d2e1c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will now implement one of TF-IDFs variation to compute from the bag of words the more relevant words. The formulation you should use is one you've learned before:\n",
    "\n",
    "$$ tfidf _{t, d} =(tf_{t,d})*(log_2{(1 + \\frac{N}{df_{t}})})  $$\n",
    "\n",
    "Implement the TF-IDF below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb6996cb41762895",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(BoW_df):\n",
    "    \"\"\"\n",
    "    Returns pandas dataframe of a tfidf representation from a BoW representation dataframe.\n",
    "\n",
    "    Args:\n",
    "    BoW_df - dataframe with document word counts (Bag of Words)\n",
    "    \"\"\"\n",
    "    # tf = (...)\n",
    "    \n",
    "    # def _idf(column):\n",
    "    #   return (...)\n",
    "    \n",
    "    # tf_idf = (...)\n",
    "    \n",
    "    # return tf_idf\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    tf = BoW_df.div(BoW_df.sum(axis=1), axis=0)\n",
    "    \n",
    "    def idf(column):\n",
    "        return np.log2(1 + len(column) / sum(column > 0))\n",
    "\n",
    "    tf_idf = (np.log2(1 + tf)).multiply(tf.apply(idf))\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ab99eef3c657a91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's now apply it to our previous BoW (note: load the BoW first if you could not use your text cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffdac98448888edf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>/&gt;&lt;</th>\n",
       "      <th>/&gt;</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>.&lt;</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>endearing</th>\n",
       "      <th>cortney</th>\n",
       "      <th>fatal</th>\n",
       "      <th>incidents</th>\n",
       "      <th>erupts</th>\n",
       "      <th>semblance</th>\n",
       "      <th>thirds</th>\n",
       "      <th>miserable</th>\n",
       "      <th>shoes</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   br  /><  />  movie  film  .<  one  like  time  even  ...   endearing  \\\n",
       "0   0    0   0      3     5   0    1     0     2     0  ...           0   \n",
       "1   0    0   0      1     0   0    0     0     0     0  ...           0   \n",
       "2   5    2   4      6     0   2    2     1     0     1  ...           0   \n",
       "3   0    0   0      4     0   0    1     1     1     1  ...           0   \n",
       "4   8    3   6      1     0   2    2     0     1     0  ...           0   \n",
       "\n",
       "   cortney  fatal  incidents  erupts  semblance  thirds  miserable  shoes  \\\n",
       "0        0      0          0       0          0       0          0      0   \n",
       "1        0      0          0       0          0       0          0      0   \n",
       "2        0      0          0       0          0       0          0      0   \n",
       "3        0      0          0       0          0       0          0      0   \n",
       "4        0      0          0       0          0       0          0      0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 7441 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW = pd.read_csv('data/imdb_sentiment_bow_sample.csv')\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b8f988a69678772",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "relevance = tfidf(BoW)\n",
    "\n",
    "assert(math.isclose(relevance['movie'][0], 0.009717385023827248),\n",
    "       math.isclose(relevance['film'][10], 0.019778475747522496),\n",
    "       math.isclose(relevance['nice'][16], 0.010851136310680626),\n",
    "       math.isclose(relevance['good'][128], 0.00989061193998239))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>/&gt;&lt;</th>\n",
       "      <th>/&gt;</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>.&lt;</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>endearing</th>\n",
       "      <th>cortney</th>\n",
       "      <th>fatal</th>\n",
       "      <th>incidents</th>\n",
       "      <th>erupts</th>\n",
       "      <th>semblance</th>\n",
       "      <th>thirds</th>\n",
       "      <th>miserable</th>\n",
       "      <th>shoes</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.017572</td>\n",
       "      <td>0.026168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016102</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         br       /><        />     movie      film        .<       one  \\\n",
       "0  0.000000  0.000000  0.000000  0.007506  0.013568  0.000000  0.002242   \n",
       "1  0.000000  0.000000  0.000000  0.018637  0.000000  0.000000  0.000000   \n",
       "2  0.019208  0.008806  0.017572  0.026168  0.000000  0.009170  0.007843   \n",
       "3  0.000000  0.000000  0.000000  0.020108  0.000000  0.000000  0.004517   \n",
       "4  0.016102  0.006918  0.013811  0.002297  0.000000  0.004805  0.004110   \n",
       "\n",
       "       like      time      even  ...   endearing  cortney  fatal  incidents  \\\n",
       "0  0.000000  0.006301  0.000000  ...         0.0      0.0    0.0        0.0   \n",
       "1  0.000000  0.000000  0.000000  ...         0.0      0.0    0.0        0.0   \n",
       "2  0.005126  0.000000  0.005862  ...         0.0      0.0    0.0        0.0   \n",
       "3  0.005898  0.006351  0.006745  ...         0.0      0.0    0.0        0.0   \n",
       "4  0.000000  0.002891  0.000000  ...         0.0      0.0    0.0        0.0   \n",
       "\n",
       "   erupts  semblance  thirds  miserable  shoes  mail  \n",
       "0     0.0        0.0     0.0        0.0    0.0   0.0  \n",
       "1     0.0        0.0     0.0        0.0    0.0   0.0  \n",
       "2     0.0        0.0     0.0        0.0    0.0   0.0  \n",
       "3     0.0        0.0     0.0        0.0    0.0   0.0  \n",
       "4     0.0        0.0     0.0        0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 7441 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-779badbcfbbc934e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.d)\n",
    "\n",
    "Now, let's use scikit-learn to get to a similar matrix and relevance numbers. Load the full processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40fe043ee73e9a82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_preprocessed = pd.read_csv('data/imdb_sentiment_processed.csv')\n",
    "\n",
    "# Get the processed text \n",
    "docs = df_preprocessed['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61ec1d1af055ee98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Start by transforming your documents into a matrix of tf-idf scores using sklearn. Make use of the `CountVectorizer` and the `TfidfTransformer` provided by scikitlearn. Implement a function that provided with a list of documents returns the word term frequency matrix and the corresponding vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8300a0a7bebf4efd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_word_term_frequency_matrix(docs):\n",
    "    \"\"\"\n",
    "    Returns the matrix of word and tf-idf scores \n",
    "    \n",
    "    Args:\n",
    "    docs - list of documents in dataset\n",
    "    \"\"\"\n",
    "    # vectorizer = ...\n",
    "    # word_count_matrix = ...\n",
    "    # vocabulary = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    vectorizer.fit(docs)\n",
    "    \n",
    "    word_count_matrix = vectorizer.transform(docs)\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    \n",
    "    # tfidf = ...\n",
    "    # word_term_frequency_matrix = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    tfidf = TfidfTransformer()\n",
    "    \n",
    "    tfidf.fit(word_count_matrix)\n",
    "    \n",
    "    word_term_frequency_matrix = tfidf.transform(word_count_matrix)\n",
    "\n",
    "    return (word_term_frequency_matrix, vocabulary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b548aa7912824b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now get the corresponding string of the most important word of this document (with index `321`) according to TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d00746278a646fe0",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "index = 321\n",
    "\n",
    "word_term_frequency_matrix, vocabulary = build_word_term_frequency_matrix(docs)\n",
    "\n",
    "max_word_idx = word_term_frequency_matrix[index].argmax()\n",
    "inv_vocab = {v: k for k, v in vocabulary.items()}\n",
    "most_relevant_word = inv_vocab[max_word_idx]\n",
    "\n",
    "assert(most_relevant_word == 'dull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ã¼ber         38813\n",
       "Ã¸stbye       38812\n",
       "Ã­snt         38811\n",
       "Ã©tc          38810\n",
       "Ã©tat         38809\n",
       "Ã©lan         38808\n",
       "Ã©cran        38807\n",
       "Ã¥ge          38806\n",
       "Ã¡ngela       38805\n",
       "Ã¡nd          38804\n",
       "zzzzzzzz     38803\n",
       "zzzzz        38802\n",
       "zzzz         38801\n",
       "zwick        38800\n",
       "zurich       38799\n",
       "zunz         38798\n",
       "zuniga       38797\n",
       "zumhofe      38796\n",
       "zulu         38795\n",
       "zukhov       38794\n",
       "zuf          38793\n",
       "zuckers      38792\n",
       "zuckerman    38791\n",
       "zucker       38790\n",
       "zucco        38789\n",
       "zu           38788\n",
       "zorro        38787\n",
       "zorrilla     38786\n",
       "zorich       38785\n",
       "zoran        38784\n",
       "             ...  \n",
       "102             29\n",
       "101st           28\n",
       "101             27\n",
       "100b            26\n",
       "1000            25\n",
       "100             24\n",
       "10              23\n",
       "0s              22\n",
       "087             21\n",
       "08              20\n",
       "07              19\n",
       "06              18\n",
       "050             17\n",
       "05              16\n",
       "04              15\n",
       "03              14\n",
       "029             13\n",
       "02              12\n",
       "01              11\n",
       "00s             10\n",
       "00am             9\n",
       "0093638          8\n",
       "0083             7\n",
       "0080             6\n",
       "0079             5\n",
       "007              4\n",
       "001              3\n",
       "00015            2\n",
       "000              1\n",
       "00               0\n",
       "Length: 38814, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary\n",
    "pd.Series(vocabulary).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab54cc6297c36ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.e)\n",
    "\n",
    "Finally, let's try to classify the sentiment of these movie reviews. \n",
    "\n",
    "Build a Pipeline to classify a review as positive or negative. Use `MultinomialNB` as your final classifier, train it and get an accuracy score above 86% on the imdb validation dataset, by choosing the best set of parameters of `CountVectorizer()` and `TfidfTransformer()`, according to what we learned in Part III.\n",
    "\n",
    "Hint: Try to use more than unigrams! Also, remember what we said about stopwords and feature space size in Part III of the Learning Notebooks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca9cc0a788f3d721",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df_preprocessed, test_size=0.3, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0712ceb628e6ed6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(train_df, validation_df):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with its \n",
    "    current accuracy in the validation set. Assume the documents are already \n",
    "    preprocessed\n",
    "    \n",
    "    Args:\n",
    "    train_df - dataframe with training docs\n",
    "    validation_df - dataframe with validation docs\n",
    "    \"\"\"\n",
    "        \n",
    "    # Build the pipeline\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    regex_list = [(\"<[^>]*>\", \"\")]\n",
    "    \n",
    "    #('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(stop_words=None, ngram_range=(1,3), max_features= 40000)),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "    \n",
    "    # Train the classifier\n",
    "    text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "    predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "    acc = np.mean(predicted == validation_df['sentiment'])\n",
    "    \n",
    "    return text_clf, acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07b6f3694941ac81",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "_, acc = train_and_validate(train_df, validation_df)\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "assert(acc >= 0.86)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
