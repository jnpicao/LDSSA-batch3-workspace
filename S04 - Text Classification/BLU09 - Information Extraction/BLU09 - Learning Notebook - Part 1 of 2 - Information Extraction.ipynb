{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Information Extraction\n",
    "\n",
    "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Information Extraction may be presented in three subtasks:\n",
    "\n",
    "* **Named Entity Recognition**, retrieve entities (like persons, location, etc.) in the text. \n",
    "* **Relation Extraction**, find the relation between two entities in the text.\n",
    "* **Template Filling**, find the correct entity to fill a certain template, for instance.\n",
    "\n",
    "In this BLU we are going to learn some of the basic techniques to extract specific (pre-specified) information from textual sources. From the three specified task, we are going to **focus on the task of named-entity recognition (NER)** where our objective is to **retrieve all the mentions** of entities like persons, locations, time, among others. The other two are mentioned for the sake of completeness and you should definitely research more about them, specially if you're eager to learn more about NLP.\n",
    "\n",
    "![robot entities](./media/robot_entities.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work in a corpus containing forum discussions. We extracted a sample from Reddit for this use. For more interesting examples, you may find more textual data available at https://files.pushshift.io/reddit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read 1000 documents\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('./datasets/sample_data.json') as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        docs.append(entry['body'])\n",
    "        \n",
    "print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction with Regular Expressions\n",
    "\n",
    "In BLU7, we became pros of regular expressions. We're going to try to use them to our task of recognizing entities. Take a moment to think about all the possibilities of Entities that we can find in a text. Do you think such a task will be achievable using only regular expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regex](./media/regex.gif \"regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, let's say that your boss asked you to retrieve all the **dates** mentioned in our sample corpus. We learned in BLU7 that it is easy to use a regular expression for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find all possible dates in the format xx/xx/xxxx\n",
    "data = ' '.join(docs)\n",
    "re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{2,4}[/-]\\d{1,2}[/-]\\d{1,2}', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks like it's going to be a breeze. However, now your boss decides to ask you to retrieve all the **country names** which appear in the corpus instead. \n",
    "\n",
    "One possible approach is to retrieve a list of all countries that exist and look for the occurence of such elements in the corpus. Let's try that, shall we?\n",
    "\n",
    "![alt text](./media/countries_meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "with open('./datasets/countries.txt') as fp:\n",
    "    for line in fp:\n",
    "        countries.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use again regular expressions for this. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 763, 765)\n",
      "('United States', 827, 840)\n",
      "('UK', 6971, 6973)\n",
      "('US', 7000, 7002)\n",
      "('Puerto rico', 8026, 8037)\n",
      "('us', 8638, 8640)\n",
      "('France', 19815, 19821)\n",
      "('us', 21563, 21565)\n",
      "('Puerto Rico', 27659, 27670)\n",
      "('Puerto Rico', 27754, 27765)\n",
      "('US', 28101, 28103)\n",
      "('Canada', 29439, 29445)\n",
      "('USA', 32880, 32883)\n",
      "('Norway', 34749, 34755)\n",
      "('Korea', 34837, 34842)\n",
      "('USA', 35738, 35741)\n",
      "('United States', 41060, 41073)\n",
      "('us', 42290, 42292)\n",
      "('us', 42403, 42405)\n",
      "('Soviet', 44563, 44569)\n",
      "('us', 49625, 49627)\n",
      "('Chad', 51352, 51356)\n"
     ]
    }
   ],
   "source": [
    "# Sort country list by length. This is important to match longer spans before short \n",
    "# ones (like in 'Papua New Guinea' vs. 'Papua')\n",
    "countries.sort(key=len, reverse=True)\n",
    "\n",
    "# Make a regex to recognize all possible names.\n",
    "# '|' creates the or operation in regex\n",
    "# \\b means word boundaries (punctuation or white spaces)\n",
    "# re.escape is used to escape regex operators like '.'    \n",
    "countries_regex = r'\\b(' + '|'.join([re.escape(c) for c in countries]) + r')\\b'\n",
    "\n",
    "# finditer is similar to findall\n",
    "# the flag re.I means to ignore casing (accept both lowercase and uppercase letters as the same)\n",
    "for i, m in enumerate(re.finditer(countries_regex, data, flags=re.I)):\n",
    "    print( (m.group(), m.start(), m.end()) )\n",
    "    # just show the first 20\n",
    "    if i > 20:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this approach working?**\n",
    "\n",
    "It seems like the word **'us'**, for example, has caused some confusion. It could be the country _U.S._, or just the pronoun _us_. In this case, just comparing the word form we are not able to disambiguate the two forms. We will need either more **context** or more **linguistic information** and regular expression won't give us none of that.\n",
    "\n",
    "Luckily, you already know an NLP library which can provide you the correct information to disambiguate the word 'us'. In the next examples, we will use SpaCy as our NLP toolkit to give us just that.\n",
    "\n",
    "## Deeper look in information extraction using SpaCy\n",
    "![Spacy](./media/spacy.jpg)\n",
    "\n",
    "If you remember BLU8, we used SpaCy to understand word vectors (aka word embeddings). We will make use of the medium sized SpaCy english model once again. In case you haven't downloaded it yet, here's the command once again:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "    \n",
    "But of course we could have used any english model (en_core_web_sm, en_core_web_md, en_core_web_lg) provided by SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are disabling the synctatic parser from pipeline to improve speed.\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SpaCy, we will process the documents with the complete NLP pipeline using [pipe](https://spacy.io/usage/processing-pipelines). This means that `pipe` will process our text, tokenize it and extract information from it using all the CPU cores from our machine. Concretely, it will Part-of-Speech tag (more on that later), parse and extract entities.\n",
    "\n",
    "We won't get into details on how SpaCy does this -- what matters is that it uses fast machine learning models with good enough accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while.\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to do NER (Named Entity Extraction) in a piece of text. We can get an example sentence from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR Tolkien. Gandalf, Aragorn, Frodo, Bilbo Baggins, Gollum...\n"
     ]
    }
   ],
   "source": [
    "example = docs[631]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nova launcher has so many gestures. It's so good.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"Nova launcher has so many gestures. It's so good.\",\n",
       " 'ents': [{'start': 0, 'end': 4, 'label': 'ORG'}],\n",
       " 'tokens': [{'id': 0, 'start': 0, 'end': 4, 'pos': 'PROPN', 'tag': 'NNP'},\n",
       "  {'id': 1, 'start': 5, 'end': 13, 'pos': 'NOUN', 'tag': 'NN'},\n",
       "  {'id': 2, 'start': 14, 'end': 17, 'pos': 'AUX', 'tag': 'VBZ'},\n",
       "  {'id': 3, 'start': 18, 'end': 20, 'pos': 'ADV', 'tag': 'RB'},\n",
       "  {'id': 4, 'start': 21, 'end': 25, 'pos': 'ADJ', 'tag': 'JJ'},\n",
       "  {'id': 5, 'start': 26, 'end': 34, 'pos': 'NOUN', 'tag': 'NNS'},\n",
       "  {'id': 6, 'start': 34, 'end': 35, 'pos': 'PUNCT', 'tag': '.'},\n",
       "  {'id': 7, 'start': 36, 'end': 38, 'pos': 'PRON', 'tag': 'PRP'},\n",
       "  {'id': 8, 'start': 38, 'end': 40, 'pos': 'AUX', 'tag': 'VBZ'},\n",
       "  {'id': 9, 'start': 41, 'end': 43, 'pos': 'ADV', 'tag': 'RB'},\n",
       "  {'id': 10, 'start': 44, 'end': 48, 'pos': 'ADJ', 'tag': 'JJ'},\n",
       "  {'id': 11, 'start': 48, 'end': 49, 'pos': 'PUNCT', 'tag': '.'}]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 15\n",
    "print(docs[idx])\n",
    "docs[idx].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SpaCy, it's really easy to extract entities - we can simply use `.ents` in our previously processed text, and SpaCy will use its built-in model to get the entities present in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR Tolkien 0 11 PERSON\n",
      "Gandalf 13 20 PERSON\n",
      "Aragorn 22 29 PERSON\n",
      "Frodo 31 36 PERSON\n",
      "Bilbo Baggins 38 51 PERSON\n",
      "Gollum 53 59 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in example.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example sentence, SpaCy correctly labels all these LOTR characters with the Person entity. You could further argue that Gandalf is a wizard and Frodo/Bilbo are hobbits, but let's not penalize SpaCy on that one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is processed and we know how to get entities, let's build a `Matcher` in SpaCy.\n",
    "\n",
    "A `Matcher` is SpaCy's version of a regular expression - it searches for patterns in your text, according to the rules you give it. However, it is much more powerful since it has access to the outputs of the aforementioned NLP pipeline. That means we can search patterns that include certain entities or Part-of-Speech tags. \n",
    "\n",
    "In this `Matcher` we will define templates which we will use later to match elements in the text (thus using it to do information extraction). The `Matcher` is initialized using the vocabulary object, which must be shared with the documents the matcher will operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a similar matcher as we did above with regular expressions. We are going to get each country name and add it as a pattern to the `matcher`. To add a pattern, we can simply use `.add()`. It receives:\n",
    "\n",
    "- an ID (the name we want to give our pattern)\n",
    "- a callable function that is called when there is a match (we're not going to use anything)\n",
    "- the pattern itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Build a pattern from the country name. For example: United States -> [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "    # LOWER means to match the words in the lowercased token.\n",
    "    pattern = [{'LOWER': c.lower()} for c in country.split()]\n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "69 50 51 us\n",
      "146 4 5 France\n",
      "167 29 30 us\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "320 35 36 us\n",
      "320 58 59 us\n",
      "335 38 39 Soviet\n",
      "335 38 39 Soviet\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "# for screen economy, let's just show the matches for the first 400 documents.\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, in order to disambiguate the retrieval of 'U.S.' vs 'us' we need to add more linguistic information to the `matcher`. Let's play with Part of Speech (PoS).\n",
    "\n",
    "## But what is Part-of-Speech?\n",
    "\n",
    "If you remember from your language classes, you could categorize words in a sentence according to the role they have in it. In NLP, we call this Part of Speech tags. For the English language, common PoS tags are: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.\n",
    "\n",
    "SpaCy adopts the Universal PoS tagset where any language has a common subset of PoS defined. The list of all possible values can be consulted [here](https://spacy.io/api/annotation#pos-tagging).\n",
    "\n",
    "In this case, we are interested in matching the country names that were tagged as **Proper Nouns** ('PROPN' tag obtained from the tagset list).\n",
    "\n",
    "![Pronoun meme](./media/pronoun.jpg)\n",
    "\n",
    "In SpaCy, just as entities of a document are inside `doc.ents`, for each token of a document we can find its assigned POS tag by using `.pos_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR PROPN\n",
      "Tolkien PROPN\n",
      ". PUNCT\n",
      "Gandalf PROPN\n",
      ", PUNCT\n",
      "Aragorn PROPN\n",
      ", PUNCT\n",
      "Frodo PROPN\n",
      ", PUNCT\n",
      "Bilbo PROPN\n",
      "Baggins PROPN\n",
      ", PUNCT\n",
      "Gollum PROPN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in example:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `Matcher` is pretty smart, so we only really need to add to a `'POS'` entry in the pattern dictionary and the tag we are looking for as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Pronoun.\n",
    "    pattern = [{'LOWER': c.lower(), 'POS': 'PROPN'} for c in country.split()]    \n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "146 4 5 France\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly the PoS tagger is based on a machine learning method, so it is prone to errors. Notice how it causes _Puerto rico_ of document 64 to be out of this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using complex patterns\n",
    "\n",
    "Let's now look into other types of information extraction methods which use complex structures. For example, let's say we want to extract places. Usually, places come up in text in structures similar to:\n",
    "\n",
    "* go to xx\n",
    "* went from xxx\n",
    "* going to xx\n",
    "\n",
    "**Note**: Notice that such patterns could be interesting to the task of relation extraction we mentioned in the intro. But that's something we will leave up to you to look further into.\n",
    "\n",
    "In order to build a SpaCy pattern for the proposed sentence structure, we are going to use the lemma word 'go' (remember lemmatization from BLU07? We can do this in SpaCy pretty easily as well!), which is invariant for all possible verb inflexitions, a preposition (POS tag name - ADP) and a proper noun (POS tag name - PROPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LEMMA': 'go'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]\n",
    "matcher.add('LOC', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5 go for Sorcery\n",
      "24 27 goes to GTA\n",
      "246 249 going to Osaka\n",
      "46 49 went to whataburger\n",
      "81 84 gone to Irvine\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "doc: I would go for Sorcery because of The Ultimate Hat most of the time, it sounds really cool to be in Huge Doggo form more often, 33s downtime on Ult sounds insane.\n",
      "\n",
      "If the enemy is a CC Heavy Comp I would opt for Precision, with the 'Super Dangerous Game' rune and the Tenacity one, sounds good for surviving teamfights and waves of CC.\n",
      "\n",
      "length of matches: 1\n",
      "2 5 go for Sorcery\n",
      "\n",
      "\n",
      "doc: Hi jessicamshannon,\n",
      "\n",
      "\n",
      "\n",
      "Your submission has been removed because it violates:\n",
      "\n",
      "&gt;RULE #7: Off-topic\n",
      "\n",
      "\n",
      "\n",
      "Link goes to GTA screenshot\n",
      "\n",
      "\n",
      "\n",
      "You can find all of our rules in the sidebar. Please look them over before contributing again.\n",
      "\n",
      "If you have any questions or concerns, please [message the moderators](https://www\\.reddit\\.com/message/compose?to=%2Fr%2FCrimeScene&amp;subject=about my removed submission&amp;message=I'm writing to you about the following submission: https://www.reddit.com/r/CrimeScene/comments/6ryulq/soldiers_pov_during_operation_overlord_dday_june/ %0D%0DMy issue is...). Direct replies to official mod comments will be removed.\n",
      "\n",
      "length of matches: 1\n",
      "24 27 goes to GTA\n",
      "\n",
      "\n",
      "doc: So yeah. Late last year I was travelling through Japan with some friends of mine (I tend to goto Japan for a few weeks every year) \n",
      "\n",
      "On this occasion I met this lovely 38 year old woman named Hiroko at a cocktail bar in Kyoto. I noticed her looking my way a few times but I couldn't tell if it was good or bad because, well I'm a bigger guy and in Japan it's already a big disadvantage lol \n",
      "\n",
      "With the power of a little liquid courage I managed to walk past her as I went to the to the bar and as I walked back I managed to make a little joke about the drink she had (wish I did in English, that could've been a waste of time lol) thankfully she saw the humor to it and asked where I am from \n",
      "\n",
      "So we sat and spoke for a bit, found out she actually visited my city with her now ex husband a few years ago. So we already had some common ground. In the middle of chatting causally we were making cheeky jabs at each other too so I could tell there was something there. Her girlfriends arrived and so we exchanged Line IDs and went our seperate ways.\n",
      "\n",
      "Now I didnt think this would go anywhere, because I was going to Osaka the next day. But we spoke for a couple days on Line before she asked me straight up if I wanted to spend some time with her on the weekend. Of course I jumped at the opportunity and booked us an Airbnb.\n",
      "\n",
      "She caught the train in from Kyoto and she was wearing this cute little outfit that and a little bow tie lol but she made it look cute. We headed back the Airbnb where wow it had an incredible view of Osaka Castle. \n",
      "\n",
      "We sat down on the couch to rest our feet and she didnt waste much time. She told me she was excited because she had never been with a \"western man\" before (whether thats true or not, i dont give a shit) and went straight to unzipping my pants and reaching into grab my cock. With a smile on her face she looked and me and said \"very big\" (now for record, she's clear fan servicing. Im not overly big in that area at all. Probably just abit over average in length, pretty girthy though)\n",
      "\n",
      "She told me stand up and when i did she ripped my pants down and got to her knees infront of me and started service my cock. Her gorgeous dark eyes staring up at me while she tried her best to deep throat my cock was mind blowing. Not only this, she was very attentive to my sensitive spots and would torture me with them and giggle at the sight of me shaking. \n",
      "\n",
      "After I came I was hard again a few minutes later and ready to feel her little Japanese pussy. She layed out on the bed and I got the tip of my cock slightly in before she pushed me back and told me no! No..no? I couldnt understand, she then smirked and stood up and whispered in my earth \"you must wait\" she got dressed and told me to get my clothes on too because she wanted to have dinner.\n",
      "\n",
      "I stood there with my throbbing cock, confused and aroused haha this woman was teasing the fuck out of me and I loved it. So we went back out for a couple of hours and had dinner before heading back to the airbnb. The whole time all I could think about we fucking her brains out, I think this is exactly what she wanted.\n",
      "\n",
      "We finally get back. She has a shower straight away and tells me to have one after her. After mine, i come out and see her on the bed wearing thigh highs and nothing else. They were pinching in her skin ever so much that it caused a slight dimple in the skin, so hot!\n",
      "\n",
      "She sits down on the bed and gestures me over and stops me when im leaning down to kiss her. She puts her hand on my head and pushes me down so i am level with her pussy and demands me to eat it and you bet I did. Her pussy tasted incredible and I could tell from sliding a finger into her pussy that it was gonna be tough when my cock was going to enter.\n",
      "\n",
      "After willingly complying to her demands and eating her pussy she told me that i had waited long enough and now i was allowed to enter her. We started with her on her back and her holding her legs back. It took abit of work and some lube, but that moment of penetration where her eyes shut tight, mouth gasped, head back and hands gripped the blanket beneath her is an image and feeling i'll never forget.\n",
      "\n",
      "I had to work her tight pussy slowly to begin with though and as she became more used to my girth she started to push against me and as my cock pushed a little further in each time she would let out a sigh. When we finally worked it in and we could build up a good rhythm it was amazing. This gorgeous Japanese woman taking my cock and shaking and one put she was moaning so hard i could see a vessel form on her forehead before she finally gasped for air and let go of the blankets.\n",
      "\n",
      "After seeing her let go i decided to lean down and scoop her up. Her legs dangling over my arms and i held onto her cute little ass. She put her arms around my neck and held on while I pounded fuck out of her pussy but she could only handle this for a couple of minutes because it was beginning to hurt.\n",
      "\n",
      "So I placed her back onto the bed on all fours and teased her pussy and clit from behind with my cock before slowling re-entering her incredible pussy. With her thrust in she would push back. Her ass was the tight little thing but enough meat for me to grab a handful with each hand and pound away her. \n",
      "\n",
      "There was a moment I looked up and realised how cool everything was about this too as i looked out the window and saw osaka castle lit up and i stopped for a moment, still inside her and told her to look too. She shared the same enthusiasm of how cool this experience was. We went back to push each others sexual limits and fucking like animals for the rest of night.\n",
      "\n",
      "We lazed around in bed together the next morning before we had a couple more rounds of sex and then check out. We got brunch. Went to some cool bamboo forest and shrines. We said our goodbyes and that was that. We still keep in touch but we are both busy. Hopefully when I'm in Japan next we'll be able to hook up again :)\n",
      "\n",
      "length of matches: 1\n",
      "246 249 going to Osaka\n",
      "\n",
      "\n",
      "doc: Its his own fault.  He said yesterday when he started the stream that he had woken up at 9AM that morning and hadn't done shit and how he had broken phones and stuff (use that time he didn't do shit/went to whataburger with Greek and go get your phones fixed maybe?) so thats on him for not doing shit about it.  Teradek trash?  Why hasn't he gone to Irvine again and seen what was up with his setup?  He just keeps making excuse after excuse for scuffed content/not streaming and you normies just eat that shit up.  I call a spade a spade and this dude is a fucking victim-card playing idiot.\n",
      "\n",
      "length of matches: 2\n",
      "46 49 went to whataburger\n",
      "81 84 gone to Irvine\n",
      "\n",
      "\n",
      "doc: This is a bit inconsistent with Thanos Imperative considering Robbie made it safe to Hala and was in Rich's and Star-Lord's funeral.  \n",
      "\n",
      "This preview implies he disappeared sometime before they launched their assault against the Cancerverse creatures and that Rich taking the full Nova force partially fucked him up.\n",
      "\n",
      "But seeing how much of a minor player Robbie was in Thanos Imperative I really don't mind that much if all of that was changed, especially considering where I think Duggan is going with Robbie.\n",
      "\n",
      "That said Duggan mentioned this issue is going to be heartbreaking, and I think we're also suppose to find out who Talonar is in this issue (if not we will know before October ends), coupled with the fact that ot might be the reunion between Rich and Peter, yeah I'm preparing for a tearjerker (and going by his Deadpool run Duggan can do depressing scenes).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "length of matches: 1\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) > 0:\n",
    "        print('\\n\\ndoc: {}'.format(doc))\n",
    "        print('\\nlength of matches: {}'.format(len(matches)))\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sure aren't all the locations that are present in our corpus! Not what we expected then :( \n",
    "\n",
    "Once again, we are finding out that it is very difficult to build patterns to match these type of ocurrences in the text. Addressing all possible patterns for person, location, etc. this way is very inneficient and difficult. \n",
    "\n",
    "Another possible way to go is to annotate examples in a corpus. We can train machine learning systems to automatically extract patterns from annotated corpora. Such class of machine learning methods are known as sequencial labeling and the most famous approaches are [CRFs](https://people.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf) and [Seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n",
    "\n",
    "Fortunately, as explained above, Spacy already contains pre-trained models for standard named-entities. Besides _Person_ (PER) entities like _Bilbo_ and _Organization_ (ORG) entities like _PayPal_ , we can also extract _Location_ entities with the code GPE!\n",
    "\n",
    "Let's try to extract all Locations using the built-in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 New York 0 8\n",
      "10 us 5 7\n",
      "26 yocan 47 52\n",
      "30 Portland 5 13\n",
      "30 timberline 352 362\n",
      "30 skibowl 367 374\n",
      "30 Portland 402 410\n",
      "58 UK 117 119\n",
      "58 US 146 148\n",
      "64 Puerto rico 81 92\n",
      "146 France 16 22\n",
      "156 VT 3 5\n",
      "200 Albuquerque 42 53\n",
      "202 Tennessee 107 116\n",
      "213 Puerto Rico 467 478\n",
      "213 Puerto Rico 562 573\n",
      "213 US 909 911\n",
      "226 North Moorhead 16 30\n",
      "226 Fargo 185 190\n",
      "229 Canada 16 22\n",
      "263 California 363 373\n",
      "267 USA 9 12\n",
      "303 Texas 21 26\n",
      "307 Capela 112 118\n",
      "367 Chad 30 34\n",
      "369 Chad 49 53\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':   \n",
    "            print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, let's not forget that, as in any machine learning model, we are also prone to errors in our prediction.\n",
    "\n",
    "Could we train a better model? Sure! Given a good corpus for training and the right tools we could achieve a very high accuracy. However, as this is not the objective of this BLU we are going to leave you some links if you want to learn more about this.\n",
    "\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another handy link - https://spacy.io/usage/linguistic-features - you can find here all kind of features SpaCy can extract for you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
