{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Advanced text classifiers\n",
    "\n",
    "As seen in the past, we can create models that take advantage of counts of words and tf-idf scores and that yield some pretty accurate predictions. But it is possible to make use of several additional features to improve our classifier. In this learning unit we are going to check how we could use other data extracted from our text data to determine if an e-mail is 'spam' or 'not spam' (also known as ham). We are going to use a very well known Kaggle dataset for spam detection - [Kaggle Spam Collection](https://www.kaggle.com/uciml/sms-spam-collection-dataset). \n",
    "\n",
    "![ham_or_spam](./media/ham_spam.jpg)\n",
    "\n",
    "This part will also introduce you to feature unions, a very useful way of combining different feature sets into your models. This scikit-learn class comes hand-in-hand with pipelines. Both allow you to delegate the work of combining and piping your transformer's outputs - your features - allowing you to create workflows in a very simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Spam and Ham\n",
    "\n",
    "As we mentioned before, we are going to try and come up with ways of detecting spam in the Kaggle Spam dataset. Let's load it and look into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/spam.csv', encoding='latin1')\n",
    "df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df.rename(columns={\"v1\":\"label\", \"v2\":\"message\"},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could think it should be quite easy to detect the spam text, since it is clearer to the human eye. I don't know about you, but I'm always suspicious of free stuff. There ain't no such thing as a free lunch (except for the ones in our hackathons).\n",
    "\n",
    "But by now you should also know that what seems obvious in text to us is sometimes not as easy to detect by a model. So, what kind of features could you use for this? The most obvious one is the words themselves, which you already know how to use with your bag-of-words approach - using CountVectorizer or TfIdfVectorizer.\n",
    "\n",
    "\n",
    "## 1.1 - Baseline\n",
    "\n",
    "To start with, let's look at the target class distribution, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we were to create a dumb classifier which always predicts \"ham\", we would get an accuracy of 86.6% for this dataset.\n",
    "\n",
    "Let's get our baseline with the Bag-of-words approach. Here we are going to use a RandomForestClassifier, a powerful machine learning classifier that fits very well in this problem. You may remember this estimator from SLU13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659192825112107"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split in train and validation\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                   ('classifier', RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_data['message'].values), train_data['label'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, test_data['message'].values))\n",
    "np.mean(predicted == test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powerful words, no?\n",
    "\n",
    "Our next step is to include other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Adding extra features\n",
    "\n",
    "But, beside this vectorization as a bag-of-words, let's understand if our classifier can be fed other signals we can retrieve from the text. Let's check for example the *length of the message*. We'll first compute it and add it as a feature in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['message'].map(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this feature useful?** \n",
    "\n",
    "Since this is only one numerical feature, we can just simply plot its distribution in our data. Let's evaluate the length distribution for \"Spam\" and \"Ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax_list = df.hist(column='length', by='label', bins=50,figsize=(12,4))\n",
    "ax_list[0].set_xlim((0,300))\n",
    "ax_list[1].set_xlim((0,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems quite different, right? So you would guess this feature should be helpful in your classifier.\n",
    "\n",
    "But let's actually check this feature through the use of a text classifier. Now for the tricky parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "\n",
    "If BLU7 is still fresh on you, you remember that when using pipelines we just fed it the text column. In fact, we could feed it more than one column, but the standard preprocessing applies the same preprocessing to the whole dataset. For our heterogeneous data, this doesn't quite work. \n",
    "\n",
    "So what can we do if we want to have a pipeline using several different features from several different columns? We can't apply the same methods to everything right? So first thing we can do is to create a selector transformer that simply returns the right column in the dataset by the key value(s) you pass.\n",
    "\n",
    "You can find below two such transformers: `TextSelector` for text columns and `NumberSelector` for number columns. Note that the only difference between them is the return type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we define pipelines tailored for each of our cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(\"message\")),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(\"length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the `StandardScaler`. The use of this scaler (scales the feature to zero mean and unit variance) is because we don't want to have different feature scales in our classifier. Most of classification algorithms expect the features to be in the same scale! \n",
    "\n",
    "You might be wondering now:\n",
    "\n",
    "> *How does this solve my problem... now I have two pipelines and although I can feed my whole dataset they are separate pipelines... does this help at all?*\n",
    "\n",
    "In fact, if you were to run them separately this would not be that helpful, since you would have to add the classifier at the end of each. It seems like we are missing only one piece, a way to combine steps in parallel and not in sequence. This is where feature unions come in!\n",
    "\n",
    "\n",
    "## 1.3 - Feature Unions\n",
    "\n",
    "While pipelines define a cascaded workflow, feature unions allow you to parallelize your workflows and have several transformations applied in parallel to your pipeline. The image below presents a simple pipeline, in sequence:\n",
    "\n",
    "<img src=\"./media/pipeline.png\" width=\"40%\">\n",
    "\n",
    "While the following one presents what it is called a feature union:\n",
    "\n",
    "<img src=\"./media/unions.png\" width=\"70%\">\n",
    "\n",
    "The latter is quite simple to define in scikit-learn, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Union allow use to use multiple distinct features in our classifier\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this combination of pipelines and feature unions inside a new pipeline! \n",
    "\n",
    "<img src=\"./media/pipelines_dawg.png\" width=\"45%\">\n",
    "\n",
    "We then get our final flow, from which we can extract the classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713004484304932"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split in train and validation\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data, train_data.label)\n",
    "\n",
    "preds = pipeline.predict(test_data)\n",
    "np.mean(preds == test_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new feature does help! We got a slightly improvement from a baseline that was already quite high. Nicely done. Let's now play with other more complex text features and see if we can maximize our classification score even more. \n",
    "\n",
    "## 1.4 - Advanced features\n",
    "\n",
    "What kind of features can you think of? \n",
    "\n",
    "You could start by just having the number of words, in the same way that we had the character length of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = df['message'].str.split().map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'][1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20\n",
       "1     6\n",
       "2    28\n",
       "3    11\n",
       "4    13\n",
       "5    32\n",
       "6    16\n",
       "7    26\n",
       "8    26\n",
       "9    29\n",
       "Name: message, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'][:10].str.split().map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember BLU7? Remember stopwords?  \n",
    "\n",
    "<img src=\"./media/stopwords.png\" width=\"40%\">\n",
    "\n",
    "Let's count only words that are not stopwords, since these are normally less relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['words_not_stopword'] = df['message'].apply(lambda x: len([t for t in x.split() if t not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can apply counts conditioned on other different characteristics, like counting the number of commas in the sentence or the number of words that are uppercased or capitalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commas'] = df['message'].str.count(',')\n",
    "df['upper'] = df['message'].map(lambda x: map(str.isupper, x)).map(sum)\n",
    "df['capitalized'] = df['message'].map(lambda x: map(str.istitle, x)).map(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also model the type of words by their length, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the average word length\n",
    "df['avg_word_length'] = df['message'].apply(lambda x: np.mean([len(t) for t in x.split() if t not in stop_words]) if len([len(t) for t in x.split(' ') if t not in stop_words]) > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look then at our output data frame, and all the features we added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>commas</th>\n",
       "      <th>upper</th>\n",
       "      <th>capitalized</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  words  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111     20   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155     28   \n",
       "3   ham  U dun say so early hor... U c already then say...      49     11   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61     13   \n",
       "\n",
       "   words_not_stopword  commas  upper  capitalized  avg_word_length  \n",
       "0                  16       1      3            3         4.750000  \n",
       "1                   6       0      2            2         4.000000  \n",
       "2                  23       0     10           10         5.173913  \n",
       "3                   9       0      2            2         3.666667  \n",
       "4                   8       1      2            2         4.250000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use the Feature Unions that we learned about to merge all these together. We'll split the data, create pipelines for all our new features and get their unions. Easy, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "upper =  Pipeline([\n",
    "                ('selector', NumberSelector(key='upper')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "capitalized =  Pipeline([\n",
    "                ('selector', NumberSelector(key='capitalized')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas),\n",
    "                     ('upper', upper),\n",
    "                     ('capitalized', capitalized)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended with our classifier so let's run it and get our classification score. \n",
    "\n",
    "*Drumroll, please.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748878923766816"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split in train and validation\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data, train_data.label)\n",
    "\n",
    "preds = pipeline.predict(test_data)\n",
    "np.mean(preds == test_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/sad.png\" width=\"40%\">\n",
    "\n",
    "Although we are still above the baseline, we didn't surpass by much the score of using just the text and its length. But don't despair, with all the tools from BLU7, BLU8 and the first part of this BLU you are already perfectly equipped to find yet new features and to analyze if they are good or not. Even to integrate your pipelines with dimensionality reduction techniques that might find your meaningful features among all these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Other classifiers\n",
    "\n",
    "New approaches in text processing have arised with new machine learning methods known as deep learning. The usage of deep learning methods is out of the scope for this BLU, but it is important that the reader is aware of the potential of such methods to improve over traditional machine learning algorithms. In particular, we suggest the knowledge about two different classifiers besides sklearn.\n",
    "\n",
    "* [StarSpace](https://github.com/facebookresearch/StarSpace)\n",
    "* [Vowpal Wabbit classifier](https://github.com/JohnLangford/vowpal_wabbit/wiki)\n",
    "\n",
    "### Additional Pointers\n",
    "\n",
    "* https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\n",
    "* http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "* http://michelleful.github.io/code-blog/2015/06/20/pipelines/\n",
    "* http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Final remarks\n",
    "\n",
    "And we are at the end of our NLP specialization. It saddens me, but it is time to say goodbye. \n",
    "\n",
    "Throughout these BLUs you learned:\n",
    "\n",
    "* How to process text \n",
    "* Typicall text features used in classification tasks\n",
    "* State of the art techniques to encode text\n",
    "* Methods to analyze feature importance\n",
    "* Methods to perform feature reduction\n",
    "* How to design pipelines and combine different features inside them\n",
    "\n",
    "You are now armed with several tools to perform text classification and much more in NLP. Don't forget to review all of this for the NLP hackathon, and to do your best in the Exercises.\n",
    "\n",
    "<img src=\"./media/so_long.jpg\" width=\"40%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
