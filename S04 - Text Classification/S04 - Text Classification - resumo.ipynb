{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU07 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (aka Regex)\n",
    "\n",
    "Regular expressions are sequences of characters that allow us to define search patterns. It goes by several rules and is one of the most fundamental and important concepts in computer science regarding working with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [python re library](https://docs.python.org/3/library/re.html). Using `search()` we can take a certain pattern and look for it in a text. This function will return a `Match` object, from which we can obtain the text portion that was matched by our pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet [\\[1\\]](https://regexr.com/3lvai)\n",
    "\n",
    "`.` - matches any character, except newline.\n",
    "\n",
    "`\\d, \\s \\S` - match digit, match whitespace, not whitespace.\n",
    "\n",
    "`\\b, \\B` - word, not word boundary.\n",
    "\n",
    "`[xyz]` - matches x, y or z.\n",
    "\n",
    "`[^xyz]` - matches anything that is not x, y or z.\n",
    "\n",
    "`[x-z]` - matches a character between x and z.\n",
    "\n",
    "`^xyz$` - `^` is the start of the string, `$` is the end of the string.\n",
    "\n",
    "`\\.` - use escaping to match special characters.\n",
    "\n",
    "`\\t`, `\\n` - matches tab and newline.\n",
    "\n",
    "`x*` - matches 0 or more symbols x.\n",
    "\n",
    "`x+` - matches 1 or more symbols x.\n",
    "\n",
    "`x?` - matches 0 or 1 symbol x.\n",
    "\n",
    "`.?`, `*?`, `+?`, etc - represent non-greedy search. \n",
    "\n",
    "`x{5}` - matches exactly 5 symbols x.\n",
    "\n",
    "`x{5,}` - matches 5 or more symbols x.\n",
    "\n",
    "`x{5, 8}` - matches between 5 and 8 symbols x.\n",
    "\n",
    "`xy|yz` - matches `xy` or `yz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.search(pattern, string)`**](https://docs.python.org/3/library/re.html#re.search)  \n",
    "\n",
    "Scan through string looking for the **first location** where the regular expression pattern produces a match, and return a corresponding [match object](https://docs.python.org/3/library/re.html#match-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for \"Madrid\":\n",
      "<re.Match object; span=(7, 13), match='Madrid'>\n",
      "\n",
      "Looking for \"Rome\":\n",
      "None\n",
      "\n",
      "Looking for \"Lisbon\":\n",
      "<re.Match object; span=(0, 6), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "print(\"Looking for \\\"Madrid\\\":\")\n",
    "match = re.search(\"Madrid\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Rome\\\":\")\n",
    "match = re.search(\"Rome\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Lisbon\\\":\")\n",
    "match = re.search(\"Lisbon\", text)\n",
    "print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.findall(pattern, string)`**](https://docs.python.org/3/library/re.html?highlight=findall#re.findall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to **return all the matches** to our pattern in a given text we might use the funcion findall(). In this case, the matched portions of the text will be returned, instead of the Match object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lisbon', 'Lisbon', 'Lisbon']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon\n",
      "Lisbon\n",
      "Lisbon\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.finditer(pattern, string)`**](https://docs.python.org/3/library/re.html?highlight=finditer#re.finditer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we really want the `Match` objects for some reason, `finditer()` should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Lisbon'>\n",
      "<re.Match object; span=(14, 20), match='Lisbon'>\n",
      "<re.Match object; span=(34, 40), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.MULTILINE`**](https://docs.python.org/3/library/re.html#re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specified, the pattern character `^` matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character `$` matches at the end of the string and at the end of each line (immediately preceding each newline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer', 'Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important step when dealing with text data is to _tokenize_ the data. In practice what this means is splitting the strings of a corpus into substrings. This is important because it transforms a string into parts that are more suitable to be used by the tools that exist in natural language processing. For instance, if we are working with the sentence\n",
    "\n",
    "_\"The car went too fast on the second lap. This damaged the tyres.\"_ ,\n",
    "\n",
    "would be better approached as a list,\n",
    "\n",
    "_[\"The\", \"car\", \"went\", \"too\", \"fast\", \"on\", \"the\", \"second\", \"lap\", \".\", \"This\", \"damaged\", \"the\", \"tyres\", \".\"]_ .\n",
    "\n",
    "We will be using [NLTK](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
