{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DÃºvidas  \n",
    "\n",
    "*  ver str built in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def vectorize():\n",
    "    vocabulary = build_vocabulary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split() # What a fuck is this doing here?!\n",
    "        vector = np.array([doc.count(word) for word in vocabulary])\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLU07 - Part 3 of 3**\n",
    "\n",
    "Labeling is not necessary!!!\n",
    "\n",
    "```python\n",
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References  \n",
    "\n",
    "\\[1\\] - [RegExr](https://regexr.com/3lvai)\n",
    "\n",
    "\\[2\\] - [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "\\[3\\] - [Deep Learning MIT Book](http://www.deeplearningbook.org/)\n",
    "\n",
    "\n",
    "### Word of Advice  \n",
    "\n",
    "Even though we are using NLTK library during this BLU, there are some other libraries that are commonly used and probably better. Here is a list of some to consider in your future challenges in NLP:\n",
    "\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/other-languages.html#python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU07 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (aka Regex)\n",
    "\n",
    "Regular expressions are sequences of characters that allow us to define search patterns. It goes by several rules and is one of the most fundamental and important concepts in computer science regarding working with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [python re library](https://docs.python.org/3/library/re.html). Using `search()` we can take a certain pattern and look for it in a text. This function will return a `Match` object, from which we can obtain the text portion that was matched by our pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet [\\[1\\]](https://regexr.com/3lvai)\n",
    "\n",
    "`.` - matches any character, except newline.\n",
    "\n",
    "`\\d, \\s \\S` - match digit, match whitespace, not whitespace.\n",
    "\n",
    "`\\b, \\B` - word, not word boundary.\n",
    "\n",
    "`[xyz]` - matches x, y or z.\n",
    "\n",
    "`[^xyz]` - matches anything that is not x, y or z.\n",
    "\n",
    "`[x-z]` - matches a character between x and z.\n",
    "\n",
    "`^xyz$` - `^` is the start of the string, `$` is the end of the string.\n",
    "\n",
    "`\\.` - use escaping to match special characters.\n",
    "\n",
    "`\\t`, `\\n` - matches tab and newline.\n",
    "\n",
    "`x*` - matches 0 or more symbols x.\n",
    "\n",
    "`x+` - matches 1 or more symbols x.\n",
    "\n",
    "`x?` - matches 0 or 1 symbol x.\n",
    "\n",
    "`.?`, `*?`, `+?`, etc - represent non-greedy search. \n",
    "\n",
    "`x{5}` - matches exactly 5 symbols x.\n",
    "\n",
    "`x{5,}` - matches 5 or more symbols x.\n",
    "\n",
    "`x{5, 8}` - matches between 5 and 8 symbols x.\n",
    "\n",
    "`xy|yz` - matches `xy` or `yz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.search(pattern, string)`**](https://docs.python.org/3/library/re.html#re.search)  \n",
    "\n",
    "Scan through string looking for the **first location** where the regular expression pattern produces a match, and return a corresponding [match object](https://docs.python.org/3/library/re.html#match-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for \"Madrid\":\n",
      "<re.Match object; span=(7, 13), match='Madrid'>\n",
      "\n",
      "Looking for \"Rome\":\n",
      "None\n",
      "\n",
      "Looking for \"Lisbon\":\n",
      "<re.Match object; span=(0, 6), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "print(\"Looking for \\\"Madrid\\\":\")\n",
    "match = re.search(\"Madrid\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Rome\\\":\")\n",
    "match = re.search(\"Rome\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Lisbon\\\":\")\n",
    "match = re.search(\"Lisbon\", text)\n",
    "print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.findall(pattern, string)`**](https://docs.python.org/3/library/re.html?highlight=findall#re.findall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to **return all the matches** to our pattern in a given text we might use the funcion findall(). In this case, the matched portions of the text will be returned, instead of the Match object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lisbon', 'Lisbon', 'Lisbon']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon\n",
      "Lisbon\n",
      "Lisbon\n"
     ]
    }
   ],
   "source": [
    "# pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.finditer(pattern, string)`**](https://docs.python.org/3/library/re.html?highlight=finditer#re.finditer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we really want the `Match` objects for some reason, `finditer()` should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Lisbon'>\n",
      "<re.Match object; span=(14, 20), match='Lisbon'>\n",
      "<re.Match object; span=(34, 40), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`re.MULTILINE`**](https://docs.python.org/3/library/re.html#re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specified, the pattern character `^` matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character `$` matches at the end of the string and at the end of each line (immediately preceding each newline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer', 'Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important step when dealing with text data is to _tokenize_ the data. In practice what this means is **splitting the strings of a corpus into substrings**. For instance, if we are working with the sentence\n",
    "\n",
    "> _\"The car went too fast on the second lap. This damaged the tyres.\"_ ,\n",
    "\n",
    "would be better approached **as a list**,\n",
    "\n",
    "> _[\"The\", \"car\", \"went\", \"too\", \"fast\", \"on\", \"the\", \"second\", \"lap\", \".\", \"This\", \"damaged\", \"the\", \"tyres\", \".\"]_ .\n",
    "\n",
    "\n",
    "First we will be using [NLTK](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) implementations. But the simplest tokenizer of all is [`str.split()`](https://docs.python.org/3/library/stdtypes.html?highlight=str%20split#str.split).    \n",
    "\n",
    "Ahead _tokenization_ will be impplemented automatically inside [`sklearn.feature_extraction.text`](https://scikit-learn.org/stable/modules/feature_extraction.html) tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The car went too fast.This damaged the tyres... 456 $89.7.5345.3...3456 foram-se\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`str.split()`](https://docs.python.org/3/library/stdtypes.html?highlight=str%20split#str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast.This', 'damaged', 'the', 'tyres...', '456', '$89.7.5345.3...3456', 'foram-se']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NLTK Tokenizer](https://www.nltk.org/_modules/nltk/tokenize/regexp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', '.This', 'damaged', 'the', 'tyres', '...', '456', '$89.7.5345.3...3456', 'foram', '-se']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "#tokenizer = RegexpTokenizer('\\w+|\\$\\d+\\.?\\d+|\\S+')\n",
    "#tokenizer = RegexpTokenizer('\\S+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The regular expression is used to define what enters the list and what is left behind.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are already some pre-defined implementations by taking advantage of `RegexpTokenizer`. These are:\n",
    "- `BlanklineTokenizer` - Tokenize a string using blank lines as delimiter.\n",
    "- `WordPunctTokenizer` - Tokenize a string into alphabetic and non-alphabetic characters.\n",
    "- `WhitespaceTokenizer`-  Tokenize a string using spaces, tabs and newlines as delimiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from nltk.tokenize import BlanklineTokenizer`  \n",
    "`from nltk.tokenize import WordPunctTokenizer`  \n",
    "`from nltk.tokenize import WhitespaceTokenizer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', '.', 'This', 'damaged', 'the', 'tyres', '...', '456', '$', '89', '.', '7', '.', '5345', '.', '3', '...', '3456', 'foram', '-', 'se']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast.This', 'damaged', 'the', 'tyres...', '456', '$89.7.5345.3...3456', 'foram-se']\n"
     ]
    }
   ],
   "source": [
    "print(WhitespaceTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **`WordPunctTokenizer()` is _\"similar\"_ to the first one we defined. This is what is commonly used and the default method of tokenization** that will be used when we talk about the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming means to get the \"root\" of the words.  \n",
    "We are going to use the NLTK implementation of the [snowball stemmer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.snowball.SnowballStemmer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'count', 'the', 'occurr', 'of', 'token', 'in', 'each', 'of', 'the', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'We are counting the occurrences of tokens in each of the documents.'\n",
    "\n",
    "#tokenizing\n",
    "tokenizer = WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(text)\n",
    "\n",
    "#stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = list(map(stemmer.stem, words))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative process to stemming is **lemmatization**.  \n",
    "\n",
    "Both processes share the goal of getting the root of the word [\\[7\\]](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), but they act differently. **Whereas stemming drops the suffix of words, lemmatization uses a dictionary to return the base form of words**, known as _lemma_.\n",
    "\n",
    "Using the example in the cited reference, given the word _saw_, stemming would tend to return only *s* , while lemmatization would take into account if the word was the verb or the noun, and correspondingly, return _see_ or _saw_  as the base form of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_n-grams_ correspond to sequences of n consecutive elements from a given sentence. Usually we refer to unigrams, bigrams, trigrams, four-grams, etc. according to the length of the sequence of elements.\n",
    "\n",
    "For instance, for the sentence\n",
    "\n",
    "`\"The driver made a mistake\"`,\n",
    "\n",
    "we would have:\n",
    "\n",
    "- unigrams: `The`, `driver`, `made`, `a`, `mistake`\n",
    "- bigrams: `The driver`, `driver made`, `made a`, `a mistake`\n",
    "- trigrams: `The driver made`, `driver made a`, `made a mistake`\n",
    "- four-grams: `The driver made a`, `driver made a mistake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We',), ('are',), ('counting',), ('the',), ('occurrences',), ('of',), ('tokens',), ('in',), ('each',), ('of',), ('the',), ('documents',), ('.',)]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'are'), ('are', 'counting'), ('counting', 'the'), ('the', 'occurrences'), ('occurrences', 'of'), ('of', 'tokens'), ('tokens', 'in'), ('in', 'each'), ('each', 'of'), ('of', 'the'), ('the', 'documents'), ('documents', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection through statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token vector is of type: <class 'numpy.ndarray'>\n",
      "token vector is of length: 300\n"
     ]
    }
   ],
   "source": [
    "token = nlp(\"house\")\n",
    "print(\"token vector is of type: {}\".format(type(token.vector)))\n",
    "print(\"token vector is of length: {}\".format(len(token.vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two different ways of getting the same vector\n",
    "assert np.all(token.vector == nlp.vocab[token.text].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc vector is of type: <class 'numpy.ndarray'>\n",
      "doc vector is of length: 300\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Some text is written here.\")\n",
    "print(\"doc vector is of type: {}\".format(type(doc.vector)))\n",
    "print(\"doc vector is of length: {}\".format(len(doc.vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give\n",
      "it\n",
      "back\n",
      "!\n",
      "He\n",
      "pleaded\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Give it back! He pleaded.\")\n",
    "assert len(doc) == 7\n",
    "\n",
    "for word in doc:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple function just to make it easier and avoid rewriting the same thing over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp(s).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7388624"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('house'), vec('home'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16095257"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('house'), vec('mouse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, to simplify our next examples, let's create a function that gets us the closest words to the vector that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10, dont_include_list=[]):\n",
    "    similarity_list = [(x, cosine(vec_to_check, vec(x))) for x in token_list if x not in dont_include_list]\n",
    "    \n",
    "    return sorted(similarity_list, key=lambda x: x[1], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different ways we could think of to construct a representation for complete sentences using the same vectors we have used for words. The average is a good enough approach to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return np.mean(np.array([w.vector for w in sent]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two different ways of getting the same vector\n",
    "vect1 = sentvec(\"i am against the trump administration .\")\n",
    "vect2 = nlp(\"i am against the trump administration .\").vector\n",
    "assert np.all(vect1 == vect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(sentvec(\"i am against the trump administration .\"), nlp(\"i am against the trump administration .\").vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
